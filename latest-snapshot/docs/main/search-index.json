{
    "docs": [
        {
            "location": "/examples/guides/460-topics/README",
            "text": " What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; ",
            "title": "Data Model"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. ",
            "title": "Topics Cache Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Topic for public messages. */ private NamedTopic&lt;ChatMessage&gt; topicPublic; /** * Topic for private messages. */ private NamedTopic&lt;ChatMessage&gt; topicPrivate; /** * Publisher for public messages. */ private Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.cacheconfig\", \"topics-cache-config.xml\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Session session = Session.create(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create public topic where everyone subscribes as anonymous and gets all messages // sent while they are connected topicPublic = session.getTopic(\"public-messages\", withType(ChatMessage.class)); publisherPublic = topicPublic.createPublisher(); subscriberPublic = topicPublic.createSubscriber(); Creates the public topic with the type of ChatMessage.class Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create private topic where messages are send to individuals and are via a subscriber group. // Subscribers will get messages sent offline if they have previously connected topicPrivate = session.getTopic(\"private-messages\", withType(ChatMessage.class)); publisherPrivate = topicPrivate.createPublisher(); subscriberPrivate = topicPrivate.createSubscriber(Subscriber.Name.of(userId)); Creates the private topic with the type of ChatMessage.class Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handleAsync((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handleAsync((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); String message = getMessageLog(chatMessage); // ensure we don't display a message from ourselves if (message != null) { messagesReceived.incrementAndGet(); log(message); } subscriber.receive().handleAsync((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation Re-subscribe Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Asynchronously send the message and increment the messages sent when complete Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.send(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (topicPublic.isActive()) { publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); subscriberPublic.close(); topicPublic.close(); } if (topicPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); subscriberPrivate.close(); topicPrivate.close(); } } ",
            "title": "The Chat Application"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Topic for public messages. */ private NamedTopic&lt;ChatMessage&gt; topicPublic; /** * Topic for private messages. */ private NamedTopic&lt;ChatMessage&gt; topicPrivate; /** * Publisher for public messages. */ private Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.cacheconfig\", \"topics-cache-config.xml\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Session session = Session.create(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create public topic where everyone subscribes as anonymous and gets all messages // sent while they are connected topicPublic = session.getTopic(\"public-messages\", withType(ChatMessage.class)); publisherPublic = topicPublic.createPublisher(); subscriberPublic = topicPublic.createSubscriber(); Creates the public topic with the type of ChatMessage.class Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create private topic where messages are send to individuals and are via a subscriber group. // Subscribers will get messages sent offline if they have previously connected topicPrivate = session.getTopic(\"private-messages\", withType(ChatMessage.class)); publisherPrivate = topicPrivate.createPublisher(); subscriberPrivate = topicPrivate.createSubscriber(Subscriber.Name.of(userId)); Creates the private topic with the type of ChatMessage.class Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handleAsync((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handleAsync((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); String message = getMessageLog(chatMessage); // ensure we don't display a message from ourselves if (message != null) { messagesReceived.incrementAndGet(); log(message); } subscriber.receive().handleAsync((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation Re-subscribe Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Asynchronously send the message and increment the messages sent when complete Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.send(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (topicPublic.isActive()) { publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); subscriberPublic.close(); topicPublic.close(); } if (topicPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); subscriberPrivate.close(); topicPrivate.close(); } } ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? ",
            "title": "Build and Run the Example"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " In this tutorial you have learned how use Coherence Topics. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Topics Overview and Configuration Performing Topics Operations ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " This tutorial walks through the steps to use Coherence Topics using a simple Chat Application. Table of Contents What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also What You Will Build You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . What You Need About 15 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Initial Project Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Topic for public messages. */ private NamedTopic&lt;ChatMessage&gt; topicPublic; /** * Topic for private messages. */ private NamedTopic&lt;ChatMessage&gt; topicPrivate; /** * Publisher for public messages. */ private Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.cacheconfig\", \"topics-cache-config.xml\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Session session = Session.create(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create public topic where everyone subscribes as anonymous and gets all messages // sent while they are connected topicPublic = session.getTopic(\"public-messages\", withType(ChatMessage.class)); publisherPublic = topicPublic.createPublisher(); subscriberPublic = topicPublic.createSubscriber(); Creates the public topic with the type of ChatMessage.class Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create private topic where messages are send to individuals and are via a subscriber group. // Subscribers will get messages sent offline if they have previously connected topicPrivate = session.getTopic(\"private-messages\", withType(ChatMessage.class)); publisherPrivate = topicPrivate.createPublisher(); subscriberPrivate = topicPrivate.createSubscriber(Subscriber.Name.of(userId)); Creates the private topic with the type of ChatMessage.class Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handleAsync((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handleAsync((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); String message = getMessageLog(chatMessage); // ensure we don't display a message from ourselves if (message != null) { messagesReceived.incrementAndGet(); log(message); } subscriber.receive().handleAsync((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation Re-subscribe Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Asynchronously send the message and increment the messages sent when complete Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.send(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (topicPublic.isActive()) { publisherPublic.send(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); subscriberPublic.close(); topicPublic.close(); } if (topicPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); subscriberPrivate.close(); topicPrivate.close(); } } Build and Run the Example Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? Summary In this tutorial you have learned how use Coherence Topics. See Also Topics Overview and Configuration Performing Topics Operations ",
            "title": "Topics"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. ",
            "title": "Coherence NamedMap "
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Void&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Obtain a NamedMap Instance"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Void&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionaly there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . ",
            "title": "A Quick Word About Serialization"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } ",
            "title": "The Put Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . ",
            "title": "The Get Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . ",
            "title": "Get Multiple Values"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . ",
            "title": "The Remove Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . ",
            "title": "The Remove Mapping Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. ",
            "title": " NamedCache Transient Data"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " This guide walks you through the basic CRUD operations on a Coherence NamedMap . What You Will Build The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Coherence NamedMap The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. Create the Test Class The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Void&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. A Quick Word About Serialization In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionaly there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . The Put Method The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } The Get Method We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . Get Multiple Values The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . The Remove Method We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . The Remove Mapping Method An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . NamedCache Transient Data As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. Summary You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Put Get and Remove Operations"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": "",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": "",
            "title": "Sub-Heading"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": "",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": "",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What You Will Build What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Sub-Heading Summary See Also ",
            "title": "Querying"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/internal/template/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/internal/template/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "Sub-Heading"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "Summary"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "See Also"
        },
        {
            "location": "/examples/internal/template/README",
            "text": " What You Will Build What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Sub-Heading Summary See Also ",
            "title": "Title"
        },
        {
            "location": "/coherence-mp/README",
            "text": " Coherence provides a number of additional modules that provide support for different Microprofile APIs. Microprofile Config Using Coherence as a Microprofile config source. Microprofile Metrics Configure Coherence to publish metrics via the Microprofile metrics API. ",
            "title": "Coherence MP"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " Note The documentation on this site covers new features and improvements that are currently only available in the open source Coherence Community Edition (CE). For complete documentation covering all the features that are available both in the latest commercial editions (Enterprise and Grid Edition) and the Community Edition, please refer to the Official Documentation . Coherence is scalable, fault-tolerant, cloud-ready, distributed platform for building grid-based applications and reliably storing data. The product is used at scale, for both compute and raw storage, in a vast array of industries such as critical financial trading systems, high performance telecommunication products, and eCommerce applications. Typically, these deployments do not tolerate any downtime and Coherence is chosen due its novel features in death detection, application data evolvability, and the robust, battle-hardened core of the product that enables it to be seamlessly deployed and adapted within any ecosystem. At a high level, Coherence provides an implementation of the familiar Map&lt;K,V&gt; interface but rather than storing the associated data in the local process, it is partitioned (or sharded) across a number of designated remote nodes. This partitioning enables applications to not only distribute (and therefore scale) their storage across multiple processes, machines, racks, and data centers, but also to perform grid-based processing to truly harness the CPU resources of the machines. The Coherence interface NamedMap&lt;K,V&gt; (an extension of Map&lt;K,V&gt; provides methods to query, aggregate (map/reduce style), and compute (send functions to storage nodes for locally executed mutations) the data set. These capabilities, in addition to numerous other features, enable Coherence to be used as a framework to write robust, distributed applications. ",
            "title": "Overview"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " assistant Coherence What is Oracle Coherence? fa-rocket Quick Start A quick-start guide to using Coherence. fa-graduation-cap Guides & Tutorials Guides, examples and tutorial about Coherence features and best practice. import_contacts Docs Oracle Coherence commercial edition product documentation. library_books API Docs Browse the Coherence CE API Docs. ",
            "title": "Get Going"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " fa-cubes Core Coherence Core Improvements. extension CDI Coherence CDI extensions. fa-cogs Microprofile Coherence Microprofile support. settings_ethernet gRPC Coherence gRPC server and client. ",
            "title": "New Features"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " fa-plug Plugins Build tool plugins to aid Coherence application development. fa-th Container Images Example Coherence OCI container (Docker) images. ",
            "title": "Tools"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " The POF Maven Plugin provides automated instrumentation of classes with the @PortableType annotation to generate consistent (and correct) implementations of Evolvable POF serialization methods. It is a far from a trivial exercise to manually write serialization methods that support serializing inheritance hierarchies that support the Evolvable concept. However, with static type analysis these methods can be deterministically generated. This allows developers to focus on business logic rather than implementing boilerplate code for the above-mentioned methods. Please see Portable Types documentation for more information and detailed instructions on Portable Types creation and usage. ",
            "title": "POF Maven Plugin"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " In order to use the POF Maven Plugin, you need to declare it as a plugin dependency in your pom.xml : <markup lang=\"xml\" > &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; An example Person class (below) when processed with the plugin is below results in the bytecode shown below. <markup lang=\"java\" >@PortableType(id=1000) public class Person { public Person() { } public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Generated bytecode: <markup lang=\"bash\" >$ javap Person.class Compiled from \"Person.java\" public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. ",
            "title": "Usage"
        },
        {
            "location": "/examples/README",
            "text": " These guides and tutorials are designed to help you be productive as quickly as possible in whatever use-case you are building with Coherence. Coherence has a long history and having been around for twenty years its APIs have evolved over that time. Occasionally there are multiple ways to implement a specific use-case, typically because to remain backwards compatible with older releases, features cannot be removed from the product. For that reason these guides use the latest Coherence versions and best practice and approaches recommended by the Coherence team for that version. explore Simple Guides fa-graduation-cap Tutorials ",
            "title": "Overview"
        },
        {
            "location": "/examples/README",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor it you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Put Get and Remove A guide showing basic CRUD put , get , and remove operations on a NamedMap . Queries A guide to the basic querying APIs in NamedMap and NamedCache . Topics A guide to using Caching Data Stores Cache Stores This guide walks you through how to use and configure Cache Stores Near Caching This guide walks you through how to use near caching within Coherence ",
            "title": "Guides"
        },
        {
            "location": "/examples/README",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. ",
            "title": "Tutorials"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Run the Completed Tutorial Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " About 30-45 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. ",
            "title": "Getting Started"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi and coherence-cdi-server are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize the Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@statware.com', address='Address 3', balance=0.0} ===ORDERS=== .... ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Configure MicroProfile GraphQL"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi and coherence-cdi-server are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize the Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@statware.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Follow the Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@statware.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") public Collection&lt;Order&gt; getOrders() { return orders.values(); } In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" ",
            "title": "Create Queries to Show Customer and Orders"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@statware.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... ",
            "title": "Inject Related Objects"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the the {@link Order} was not found */ @Mutation public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. ",
            "title": "Add Mutations"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Finally we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } ",
            "title": "Add a Dynamic Where Clause"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw clean package Run with Gradle <markup lang=\"bash\" >./gradlew runApp ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerI customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw clean package Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerI customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Completed Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. ",
            "title": "Summary"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "See Also"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial walks through the steps to enable access to Coherence data from GraphQL using Helidon’s MicroProfile (MP) GraphQL support and Coherence CDI . Table of Contents What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Run the Completed Tutorial Summary See Also What You Will Build You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . What You Need About 30-45 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Getting Started This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. Follow the Tutorial Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi and coherence-cdi-server are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize the Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@statware.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. Create Queries to Show Customer and Orders Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@statware.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") public Collection&lt;Order&gt; getOrders() { return orders.values(); } In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" Inject Related Objects From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@statware.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... Add Mutations In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the the {@link Order} was not found */ @Mutation public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. Add a Dynamic Where Clause Finally we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } Run the Completed Tutorial Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw clean package Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerI customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } Summary In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. See Also Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "GraphQL"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " In order to use Coherence gRPC client, you need to declare it as a dependency in your pom.xml . There also needs to be a corresponding Coherence server running the gRPC proxy for the client to connect to. <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " The Coherence Java gRPC Client is a library that allows a Java application to connect to a Coherence gRPC proxy server. Usage In order to use Coherence gRPC client, you need to declare it as a dependency in your pom.xml . There also needs to be a corresponding Coherence server running the gRPC proxy for the client to connect to. <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Coherence Java gRPC Client"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " Client gRPC sessions to can be configured using system properties. By default, if no properties are provided a gRPC session named default will be configured to connect to localhost:1408 . For example, the code below will create a gRPC Session named default : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder().build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); ",
            "title": "Obtain a Remote Session"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " Client sessions can be named, by providing a name to the configuration builder: For example, the code below will create a gRPC Session named foo : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder(\"foo\").build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); ",
            "title": "Named gRPC Sessions"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " Client gRPC sessions can be configured using system properties. The system property names follow the format coherence.grpc.channels.&lt;name&gt;.xyz , where &lt;name&gt; is replaced with the session name. For example the property to set the the port to connect to is coherence.grpc.channels.&lt;name&gt;.port , so to configure the port for the default session to 9099, set the property -Dcoherence.grpc.channels.default.port=9099 Property Description coherence.grpc.channels.&lt;name&gt;.host The host name to connect to coherence.grpc.channels.&lt;name&gt;.port The port to connect to coherence.grpc.channels.&lt;name&gt;.target As an alternative to setting the host and port, setting target creates a channel using the ManagedChannelBuilder.forTarget(target); method (see the gRPC Java documentation). (replace &lt;name&gt; with the session name being configured). ",
            "title": "Session Configuration via Properties"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " By default, the Channel used by the gRPC session will be configured as a plain text connection. TLS can be configured by setting the required properties. Property Description coherence.grpc.channels.&lt;name&gt;.credentials Set to one of plaintext , insecure or tls . The default is plaintext and will create an insecure plain text channel. Using insecure will enable TLS but not verify the server certificate (useful in testing). Using tls will enable TLS on the client. coherence.grpc.channels.&lt;name&gt;.tls.ca The location of a CA file if required to verify the server certs. (replace &lt;name&gt; with the session name being configured). If the server has been configured for mutual verification the client&#8217;s key and certificate can also be provided: Property Description coherence.grpc.channels.&lt;name&gt;.tls.cert The location of a client certificate file. coherence.grpc.channels.&lt;name&gt;.tls.key The location of a client key file. coherence.grpc.channels.&lt;name&gt;.tls.password The optional password for the client key file. (replace &lt;name&gt; with the session name being configured). ",
            "title": "Using TLS"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " When client code has finished with a Session it can be closed to free up and close any gRPC requests that are still open by calling the session.close() method. This will also locally release (but not destroy) all Coherence resources manged by that Session . ",
            "title": "Close a Session"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " If a fully custom channel configuration is required application code can configure the session with a Channel . <markup lang=\"java\" >Channel channel = ManagedChannelBuilder.forAddress(\"localhost\", 1408) .usePlaintext() .build(); SessionConfiguration config = GrpcSessionConfiguration.builder(channel) .named(\"foo\") .build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); The example above creates a simple gRPC channel to connect to localhost:1408 . A Session has been created with this channel by specifying the GrpcSessions.channel(channel) option. Calls to Session.create() with the same parameters, in this case channel, will return the same Session instance. Most gRPC Channel implementations do not implement an equals() method, so the same Session will only be returned for the exact same Channel instance. Close a Session When client code has finished with a Session it can be closed to free up and close any gRPC requests that are still open by calling the session.close() method. This will also locally release (but not destroy) all Coherence resources manged by that Session . ",
            "title": "Create a Session with a Custom Channel"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " The Serializer used by the remote session will default to Java serialization, unless the system property coherence.pof.enabled is set to true , in which case POF will be used for the serializer. The serializer for a session can be set specifically when creating a Session . <markup lang=\"java\" >Serializer serializer = new JsonSerializer(); String format = \"json\"; SessionConfiguration config = GrpcSessionConfiguration.builder() .withSerializer(serializer, format) .build(); Session session = Session.create(config); Session session = Session.create(GrpcSessions.channel(channel), GrpcSessions.serializer(ser, format)); In the example above a json serializer is being used. The GrpcSessions.serializer(ser, format) session option is used to specify the serializer and its format name. The format name will be used by the server to select the correct server side serializer to process the session requests and responses. The serializer format configured must also have a compatible serializer available on the server so that the server can deserialize message payloads. ",
            "title": "Specify a Serializer"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " In most cases a Coherence server only has a single ConfigurableCacheFactory , but it is possible to run multiple and hence multiple different cache services managed by a different ConfigurableCacheFactory . Typically, a scope name will be used to isolate different ConfigurableCacheFactory instances. A gRPC client session can be created for a specific server side scope name by specifying the scope as an option when creating the session. <markup lang=\"java\" >SessionConfiguration config = GrpcSessionConfiguration.builder() .withScopeName(\"foo\") .build(); Session session = Session.create(config); In the example above the GrpcSessions.scope(\"foo\") option is used to specify that the Session created should connect to resources on the server managed by the server side Session with the scope foo . ",
            "title": "Specify a Scope Name"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " The simplest way to access remote Coherence resources, such as a NamedMap when using the gRPC client is via a Coherence Session . Obtain a Remote Session Client gRPC sessions to can be configured using system properties. By default, if no properties are provided a gRPC session named default will be configured to connect to localhost:1408 . For example, the code below will create a gRPC Session named default : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder().build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); Named gRPC Sessions Client sessions can be named, by providing a name to the configuration builder: For example, the code below will create a gRPC Session named foo : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder(\"foo\").build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); Session Configuration via Properties Client gRPC sessions can be configured using system properties. The system property names follow the format coherence.grpc.channels.&lt;name&gt;.xyz , where &lt;name&gt; is replaced with the session name. For example the property to set the the port to connect to is coherence.grpc.channels.&lt;name&gt;.port , so to configure the port for the default session to 9099, set the property -Dcoherence.grpc.channels.default.port=9099 Property Description coherence.grpc.channels.&lt;name&gt;.host The host name to connect to coherence.grpc.channels.&lt;name&gt;.port The port to connect to coherence.grpc.channels.&lt;name&gt;.target As an alternative to setting the host and port, setting target creates a channel using the ManagedChannelBuilder.forTarget(target); method (see the gRPC Java documentation). (replace &lt;name&gt; with the session name being configured). Using TLS By default, the Channel used by the gRPC session will be configured as a plain text connection. TLS can be configured by setting the required properties. Property Description coherence.grpc.channels.&lt;name&gt;.credentials Set to one of plaintext , insecure or tls . The default is plaintext and will create an insecure plain text channel. Using insecure will enable TLS but not verify the server certificate (useful in testing). Using tls will enable TLS on the client. coherence.grpc.channels.&lt;name&gt;.tls.ca The location of a CA file if required to verify the server certs. (replace &lt;name&gt; with the session name being configured). If the server has been configured for mutual verification the client&#8217;s key and certificate can also be provided: Property Description coherence.grpc.channels.&lt;name&gt;.tls.cert The location of a client certificate file. coherence.grpc.channels.&lt;name&gt;.tls.key The location of a client key file. coherence.grpc.channels.&lt;name&gt;.tls.password The optional password for the client key file. (replace &lt;name&gt; with the session name being configured). Create a Session with a Custom Channel If a fully custom channel configuration is required application code can configure the session with a Channel . <markup lang=\"java\" >Channel channel = ManagedChannelBuilder.forAddress(\"localhost\", 1408) .usePlaintext() .build(); SessionConfiguration config = GrpcSessionConfiguration.builder(channel) .named(\"foo\") .build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); The example above creates a simple gRPC channel to connect to localhost:1408 . A Session has been created with this channel by specifying the GrpcSessions.channel(channel) option. Calls to Session.create() with the same parameters, in this case channel, will return the same Session instance. Most gRPC Channel implementations do not implement an equals() method, so the same Session will only be returned for the exact same Channel instance. Close a Session When client code has finished with a Session it can be closed to free up and close any gRPC requests that are still open by calling the session.close() method. This will also locally release (but not destroy) all Coherence resources manged by that Session . Specify a Serializer The Serializer used by the remote session will default to Java serialization, unless the system property coherence.pof.enabled is set to true , in which case POF will be used for the serializer. The serializer for a session can be set specifically when creating a Session . <markup lang=\"java\" >Serializer serializer = new JsonSerializer(); String format = \"json\"; SessionConfiguration config = GrpcSessionConfiguration.builder() .withSerializer(serializer, format) .build(); Session session = Session.create(config); Session session = Session.create(GrpcSessions.channel(channel), GrpcSessions.serializer(ser, format)); In the example above a json serializer is being used. The GrpcSessions.serializer(ser, format) session option is used to specify the serializer and its format name. The format name will be used by the server to select the correct server side serializer to process the session requests and responses. The serializer format configured must also have a compatible serializer available on the server so that the server can deserialize message payloads. Specify a Scope Name In most cases a Coherence server only has a single ConfigurableCacheFactory , but it is possible to run multiple and hence multiple different cache services managed by a different ConfigurableCacheFactory . Typically, a scope name will be used to isolate different ConfigurableCacheFactory instances. A gRPC client session can be created for a specific server side scope name by specifying the scope as an option when creating the session. <markup lang=\"java\" >SessionConfiguration config = GrpcSessionConfiguration.builder() .withScopeName(\"foo\") .build(); Session session = Session.create(config); In the example above the GrpcSessions.scope(\"foo\") option is used to specify that the Session created should connect to resources on the server managed by the server side Session with the scope foo . ",
            "title": "Access Coherence Resources"
        },
        {
            "location": "/examples/tutorials/000-overview",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. ",
            "title": "Tutorials"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Coherence gRPC provides the protobuf definitions necessary to interact with a Coherence data management services over gRPC. This library also provides utilities for making low-level cache requests over gRPC, converting between gRPC and Coherence binary implementations. Given this, unless there is a plan to develop a Coherence gRPC client in another language or to create new services in Java, there is little need for developers to depend on this library. ",
            "title": "Coherence gRPC"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " In order to use Coherence gRPC, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Proto File Usage services.proto defines the RPCs for interacting with a Coherence data management services requests.proto defines the request/response structs for making requests to and receiving responses from Coherence data management services ",
            "title": "Protobuf Definitions"
        },
        {
            "location": "/docs/core/03_parallel_recovery",
            "text": " Coherence introduced a disk-based persistence feature from version 12.2.1. This feature accommodates for the loss of an entire cluster, and/or simultaneous loss of a primary and backup ensuring data is recovered from disk and made available. This process of making data available is parallel across the cluster with each storage member recovering a fair share of partitions. While this recovery is in parallel across different members/processes, each member uses a single thread to recover. As of version 20.12, Coherence now recovers data in parallel within a member/process as well as in parallel across the cluster. This allows the cluster, and more importantly the associated data, to be made available as quickly as possible. Ultimately the goal is to have recovery be only limited by the throughput and latency of underlying device. Therefore this feature does assume increasing usage of the device (by accessing data in parallel) will provide some benefit and reduce the overall time to recover data. The number of threads Coherence uses can be tweaked by the following JVM argument: coherence.distributed.persistence.recover.threads This argument may be removed in a future release as we work towards deriving an optimal value. The default value is based on the following formula: num-cores * (max-heap-size / machine-memory) + 2 ",
            "title": "Parallel Recovery"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example Pluggable Cache Stores ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. ",
            "title": "CacheLoader and CacheStore Interfaces"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). ",
            "title": "Simple CacheLoader"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll ",
            "title": "Simple CacheStore"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Enable Write Behind"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Simple Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One ",
            "title": "File Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSQLDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=initial] Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=load1] You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=load2] You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=remove] Issue a get for another customer and then update the customer details. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=update] Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=addRemove] Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=loadData] ",
            "title": "HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSQLDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=initial] Issue a get for customer 1 and log the time to load <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=readThrough1] Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=readThrough2] The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=readThrough3] Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms ",
            "title": "Refresh Ahead HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this final HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreWriteBehindTest.java[tag=initial] Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreWriteBehindTest.java[tag=insert] Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreWriteBehindTest.java[tag=wait] You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries ",
            "title": "Write Behind HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } @Test public void testHibernateCacheStore() throws SQLException { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"Person\", TypeAssertion.withTypes(Long.class, Person.class)); Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); ",
            "title": "Pluggable Cache Stores"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " You have seen how to use and configure Cache Stores within Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This guide walks you through how to use and configure Cache Stores within Coherence. Coherence supports transparent read/write caching of any data source, including databases, web services, packaged applications and file systems; however, databases are the most common use case. As shorthand, \"database\" is used to describe any back-end data source. Effective caches must support both intensive read-only and read/write operations, and for read/write operations, the cache and database must be kept fully synchronized. To accomplish caching of data sources, Coherence supports Read-Through, Write-Through, Refresh-Ahead and Write-Behind caching. See the Coherence Documentation for detailed information on Cache Stores. Table of Contents What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example Pluggable Cache Stores What You Will Build This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. CacheLoader and CacheStore Interfaces Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. Simple Cache Store Example Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. File Cache Store Example In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One HSQLDb Cache Store Example In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSQLDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=initial] Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=load1] You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=load2] You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=remove] Issue a get for another customer and then update the customer details. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=update] Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=addRemove] Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreTest.java[tag=loadData] Refresh Ahead HSQLDb Cache Store Example In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSQLDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=initial] Issue a get for customer 1 and log the time to load <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=readThrough1] Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=readThrough2] The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreExpiringTest.java[tag=readThrough3] Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms Write Behind HSQLDb Cache Store Example In this final HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreWriteBehindTest.java[tag=initial] Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreWriteBehindTest.java[tag=insert] Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >Unresolved directive in README.adoc - include::src/test/java/com/oracle/coherence/guides/cachestores/HSQLDbCacheStoreWriteBehindTest.java[tag=wait] You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries Pluggable Cache Stores A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } @Test public void testHibernateCacheStore() throws SQLException { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"Person\", TypeAssertion.withTypes(Long.class, Person.class)); Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); Summary You have seen how to use and configure Cache Stores within Coherence. See Also Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "Cache Stores"
        },
        {
            "location": "/examples/guides/000-overview",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor it you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Put Get and Remove A guide showing basic CRUD put , get , and remove operations on a NamedMap . Queries A guide to the basic querying APIs in NamedMap and NamedCache . Topics A guide to using Caching Data Stores Cache Stores This guide walks you through how to use and configure Cache Stores Near Caching This guide walks you through how to use near caching within Coherence ",
            "title": "Guides"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": "",
            "title": "Coherence Java CDI gRPC Client"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. ",
            "title": "Configure gRPC Channels"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence gRPC client will attempt to connect to a default server endpoint on localhost:1409 if no other channel has been configured. This is fine for development and testing but in most real-world applications the client will need to know the endpoint to connect to. Most applications would only require a channel to connect to a single Coherence cluster but there are use-cases where clients connect to multiple clusters, and the Coherence gRPC Java client supports these use-cases too. The Coherence gRPC client has been built on top of the Helidon Microprofile gRPC library and uses it to provide gRPC channels. Configure gRPC Channels Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. ",
            "title": "Remote Connections"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. ",
            "title": "Session Types"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " Coherence uses the concept of a Session to manage a set of related Coherence resources, such as maps, caches, topics, etc. When using the Coherence Java gRPC client a Session connects to a specific gRPC channel (described above) and uses a specific serialization format to marshal requests and responses. This means that different sessions using different serializers may connect to the same server endpoint. Typically, for efficiency the client and server would be configured to use matching serialization formats to avoid deserialization of data on the server but this does not have to be the case. If the server is using a different serializer for the server side caches it must be able to deserialize the client&#8217;s requests, so there must be a serializer configured on the server to match that used by the client. As with gRPC channels above, Coherence Sessions can be configured using Helidon configuration. Coherence sessions are configured in the coherence.sessions section of the configuration. Each session has its own entry in the configuration hierarchy, as shown below: <markup lang=\"yaml\" >coherence: sessions: default: serializer: pof channel: default The example above shows configuration for the default Coherence session, this is the session that will be used to provide Coherence beans when no session name has been specified for an injection point. In this example, the default session will use POF serialization and connect to the server using the default gRPC channel. The default session, if not configured, will use the default channel and will use Java serialization. As with channels, multiple sessions can be configured: <markup lang=\"yaml\" >coherence: sessions: test: serializer: pof channel: test prod: serializer: pof channel: prod # Helidon gRPC configuration grpc: channels: - name: test host: test.storage.acme.com port: 1408 - name: prod host: storage.acme.com port: 1408 In the example above, there are two Coherence sessions configured and two corresponding gRPC channels. Session Types By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. ",
            "title": "Coherence gRPC Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " A number of commonly used Coherence objects can be injected when using Java gRPC client. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In order to inject an instance of a NamedMap into your gRPC client CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >@Inject private NamedMap&lt;Long, Person&gt; people; <markup lang=\"java\" >@Inject @SesionName(\"products\") private NamedMap&lt;Long, Product&gt; products; In this example the Coherence CDI extensions will use the products session to provide the client side NamedMap backed on the server by a NamedMap called products . Other remote resources, such a NamedCache can be injected the same way: <markup lang=\"java\" >@Inject private NamedCache&lt;Long, Product&gt; products; The Coherence CDI documentation covers the different types of resources supported by CDI. When using them with the gRPC Java client. ",
            "title": "Injecting NamedMap NamedCache and Related Objects"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " If an application bean requires multiple maps or caches where the names will only be known at runtime then a Coherence com.tangosol.net.Session can be injected instead of other specific named resources. The required maps or caches can then be obtained from the Session by calling methods such as Session.getMap or Session.getCache , etc. <markup lang=\"java\" >@Inject @Name(\"products\") private Session session; The @Name qualifier has the value products , so the Session injected here will be the pre-configured Session named products . ",
            "title": "Injecting Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. ",
            "title": "Observe Events for Maps and Caches from Specific Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. ",
            "title": "Observe Events for Maps and Caches from Specific Server-side Scopes"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence NamedMap and NamedCache APIs allow implementations of MapListener to be added that will then receive events as map/cache entries get inserted, updated or deleted. When using CDI it is possible to subscribe to the same events using CDI observer methods. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } The Observes qualifier is what makes this method a standard CDI observer. The MapName qualifier determines which map/cache to observer. If this qualifier is not present events from all caches will be observed. Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. Observe Events for Maps and Caches from Specific Sessions In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. Observe Events for Maps and Caches from Specific Server-side Scopes In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Using Asynchronous Observers All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Using CDI Observers to Handle MapEvents"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence gRPC Helidon client is a CDI enabled library that allows Java clients to connect via gRPC to a Coherence proxy server. This library has a dependency on Helidon for some services. In order to use Coherence gRPC Helidon client, you need to declare it as a dependency in your pom.xml <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-helidon-client&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Using the Coherence gRPC Helidon client assumes that there is a corresponding server process running which is using the Coherence gRPC proxy service to expose the required gRPC endpoints. Once the necessary dependency is in place, the simplest way to start using it is to just inject Coherence resources into the application&#8217;s beans. A lot of the annotations and qualifiers are identical to those described in the Coherence CDI documentation. The following sections describe different injection points in more detail. Configuring gRPC Connections Configuring Coherence Remote Sessions Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache , and related objects Session Injection Using CDI Observers to Handle Coherence Map or Cache Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches owned by a specific Session Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Remote Connections The Coherence gRPC client will attempt to connect to a default server endpoint on localhost:1409 if no other channel has been configured. This is fine for development and testing but in most real-world applications the client will need to know the endpoint to connect to. Most applications would only require a channel to connect to a single Coherence cluster but there are use-cases where clients connect to multiple clusters, and the Coherence gRPC Java client supports these use-cases too. The Coherence gRPC client has been built on top of the Helidon Microprofile gRPC library and uses it to provide gRPC channels. Configure gRPC Channels Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. Coherence gRPC Sessions Coherence uses the concept of a Session to manage a set of related Coherence resources, such as maps, caches, topics, etc. When using the Coherence Java gRPC client a Session connects to a specific gRPC channel (described above) and uses a specific serialization format to marshal requests and responses. This means that different sessions using different serializers may connect to the same server endpoint. Typically, for efficiency the client and server would be configured to use matching serialization formats to avoid deserialization of data on the server but this does not have to be the case. If the server is using a different serializer for the server side caches it must be able to deserialize the client&#8217;s requests, so there must be a serializer configured on the server to match that used by the client. As with gRPC channels above, Coherence Sessions can be configured using Helidon configuration. Coherence sessions are configured in the coherence.sessions section of the configuration. Each session has its own entry in the configuration hierarchy, as shown below: <markup lang=\"yaml\" >coherence: sessions: default: serializer: pof channel: default The example above shows configuration for the default Coherence session, this is the session that will be used to provide Coherence beans when no session name has been specified for an injection point. In this example, the default session will use POF serialization and connect to the server using the default gRPC channel. The default session, if not configured, will use the default channel and will use Java serialization. As with channels, multiple sessions can be configured: <markup lang=\"yaml\" >coherence: sessions: test: serializer: pof channel: test prod: serializer: pof channel: prod # Helidon gRPC configuration grpc: channels: - name: test host: test.storage.acme.com port: 1408 - name: prod host: storage.acme.com port: 1408 In the example above, there are two Coherence sessions configured and two corresponding gRPC channels. Session Types By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. Injecting Coherence Objects into CDI Beans A number of commonly used Coherence objects can be injected when using Java gRPC client. Injecting NamedMap NamedCache and Related Objects In order to inject an instance of a NamedMap into your gRPC client CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >@Inject private NamedMap&lt;Long, Person&gt; people; <markup lang=\"java\" >@Inject @SesionName(\"products\") private NamedMap&lt;Long, Product&gt; products; In this example the Coherence CDI extensions will use the products session to provide the client side NamedMap backed on the server by a NamedMap called products . Other remote resources, such a NamedCache can be injected the same way: <markup lang=\"java\" >@Inject private NamedCache&lt;Long, Product&gt; products; The Coherence CDI documentation covers the different types of resources supported by CDI. When using them with the gRPC Java client. Injecting Sessions If an application bean requires multiple maps or caches where the names will only be known at runtime then a Coherence com.tangosol.net.Session can be injected instead of other specific named resources. The required maps or caches can then be obtained from the Session by calling methods such as Session.getMap or Session.getCache , etc. <markup lang=\"java\" >@Inject @Name(\"products\") private Session session; The @Name qualifier has the value products , so the Session injected here will be the pre-configured Session named products . Using CDI Observers to Handle MapEvents The Coherence NamedMap and NamedCache APIs allow implementations of MapListener to be added that will then receive events as map/cache entries get inserted, updated or deleted. When using CDI it is possible to subscribe to the same events using CDI observer methods. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } The Observes qualifier is what makes this method a standard CDI observer. The MapName qualifier determines which map/cache to observer. If this qualifier is not present events from all caches will be observed. Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. Observe Events for Maps and Caches from Specific Sessions In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. Observe Events for Maps and Caches from Specific Server-side Scopes In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Using Asynchronous Observers All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Usage"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. ",
            "title": "Image Contents"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 21.06-SNAPSHOT the image produced will be oraclecoherence/coherence-ce:21.06-SNAPSHOT To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:21.06-SNAPSHOT ",
            "title": "Building the Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the Image in Kubernetes"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. ",
            "title": "Specifying Coherence System Properties"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. ",
            "title": "Specifying JVM Options"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Adding to the Classpath"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This module builds an example Coherence OCI compatible image. The image built in this module is a demo and example of how to build a Coherence image using the JIB Maven Plugin . The image is not intended to be used in production deployments or as a base image, it is specifically for demos, experimentation and learning purposes. Image Contents The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. Building the Image Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 21.06-SNAPSHOT the image produced will be oraclecoherence/coherence-ce:21.06-SNAPSHOT To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:21.06-SNAPSHOT Run the image Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. Specifying Coherence System Properties Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. Specifying JVM Options Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. Adding to the Classpath Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Coherence OCI Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Multiple containers can be started to form a cluster. By default, Coherence uses multi-cast for cluster discovery but in containers this either will not work, or is not reliable, so well-known-addressing can be used. This example is going to use basic Docker commands and links between containers. There are other ways to achieve the same sort of functionality depending on the network configurations you want to use in Docker. First, determine the name to be used for the first container, in this example it will be storage-1 . Next, create a ` Start the first container in the cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-1 \\ --hostname storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} The first container has been started with a container name of storage-1 , and the host name also set to storage-1 . The container sets the WKA host name to storage-1 using -e coherence.wka=storage-1 (this will be converted to the System property coherence.wka=storage-1 see Specifying Coherence System Properties above). The container sets the Coherence cluster name to testing using -e coherence.cluster=testing (this will be converted to the System property coherence.cluster=testing see Specifying Coherence System Properties above). The important part here is that the container has a name, and the --hostname option has also been set. This will allow the subsequent cluster members to find this container. Now, subsequent containers can be started using the same cluster name and WKA host name, but with different container names and a link to the first container, all the containers will form a single Coherence cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-2 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} docker run -d -P \\ --name storage-3 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} Two more containers, storage-2 and storage-3 will now be part of the cluster. All the members must have a --link option to the first container and have the same WKA and cluster name properties. ",
            "title": "Clustering"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image comes with tracing already configured, it just requires a suitable Jaeger server to send spans to. The simplest way to start is deploy the Jaeger all-in-one server, for example: <markup lang=\"bash\" >docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:latest The Jaeger UI will be available to browse to at http://127.0.0.1:16686 Jaeger has been started with a container name of jaeger , so it will be discoverable using that host name by the Coherence containers. Start the Coherence container with a link to the Jaeger container and set the JAEGER_AGENT_HOST environment variable to jaeger : <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ oraclecoherence/coherence-ce:{version-coherence-maven} Once the Coherence container is running perform some interactions with it using one of the exposed services, i.e Extend or gRPC, and spans will be sent to the Jaeger collector and will be visible in the UI by querying for the coherence service name. The service name used can be changed by setting the JAEGER_SERVICE_NAME environment variable when starting the container, for example: <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ -e JAEGER_SERVICE_NAME=coherence-test oraclecoherence/coherence-ce:{version-coherence-maven} Spans will now be sent to Jaeger with the service name coherence-test . Tracing is very useful to show what happens under the covers for a given Coherence API call. Traces are more interesting when they come from a Coherence cluster with multiple members, where the traces span different cluster members. This can easily be done by running multiple containers with tracing enabled and configuring Clustering as described above. ",
            "title": "Tracing"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring expiry and eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units as well as optionally with an expiry. The back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays Cache MBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays Cache MBean metrics for the front cache to show cache pruning happening Displays StorageManager MBean metrics to show listener registrations There are three tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest com.oracle.coherence.guides.nearcaching.SimpleExpiringNearCachingExampleTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;expiring-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;front-expiry&lt;/param-name&gt; &lt;param-value&gt;8s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;expiry-delay&gt;{front-expiry 0s}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define cache mapping for caches matching expiring-cache-* to the near-scheme using macros to set the front limit to 100, and the expiry to 8 seconds Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define front scheme expiry-delay using the macro and defaulting to 0s if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.cacheconfig\", \"near-cache-config.xml\"); System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { Session session = Session.create(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); final int MAX = 100; Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up, and expiry to happen if we are using expiring front cache Base.sleep(5000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 5 seconds to ensure JMX stats are up to date as well as expire entries second time around if the expiring cache is being used Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 SimpleExpiringNearCachingExampleTest - uses all invalidation strategy and front expiry of 3 seconds There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. Review the SimpleExpiringNearCachingExampleTest <markup lang=\"java\" >public class SimpleExpiringNearCachingExampleTest { @Test public void testExpiringNearCache() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"expiring-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called expiring-cache-all , which matches the size limited and expiring near cache and invalidation strategy of all . Due to the expiry behaviour, the output will be different from the size limited cache. ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Run the examples using one of the methods below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest com.oracle.coherence.guides.nearcaching.SimpleExpiringNearCachingExampleTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations SimpleExpiringNearCachingExampleTest Output The output of this test is slightly different from the previous two as and expiring cache is used. See below for details of the output. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache expiring-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 21.364ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=expiring-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.2 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.543ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=expiring-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.2 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=expiring-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=110 &lt;Info&gt; (thread=main, member=1): Name: Size, value=10 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5238095238095238 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.2 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=expiring-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 21.364ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.543ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see the size is 10 as the total sleep time was &gt; 8 seconds which meant the near cache expired 100 entries before the next 10 entries were added Number of prune operations are 0 as no size limiting pruning has been done Because we are using the all invalidation strategy there is only 1 listener registered for all the entries ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured expiry and eviction policies Explored MBeans related to near caching ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This guide walks you through how to use near caching within Coherence by providing various examples and configurations that showcase the different features available. A near cache is a hybrid cache; it typically fronts a distributed cache or a remote cache with a local cache. Near cache invalidates front cache entries, using a configured invalidation strategy, and provides excellent performance and synchronization. Near cache backed by a partitioned cache offers zero-millisecond local access for repeat data access, while enabling concurrency and ensuring coherency and fail over, effectively combining the best attributes of replicated and partitioned caches. See the Coherence Documentation for detailed information on near caches. Table of Contents What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring expiry and eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Example Code The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units as well as optionally with an expiry. The back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays Cache MBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays Cache MBean metrics for the front cache to show cache pruning happening Displays StorageManager MBean metrics to show listener registrations There are three tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest com.oracle.coherence.guides.nearcaching.SimpleExpiringNearCachingExampleTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;expiring-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;front-expiry&lt;/param-name&gt; &lt;param-value&gt;8s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;expiry-delay&gt;{front-expiry 0s}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define cache mapping for caches matching expiring-cache-* to the near-scheme using macros to set the front limit to 100, and the expiry to 8 seconds Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define front scheme expiry-delay using the macro and defaulting to 0s if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.cacheconfig\", \"near-cache-config.xml\"); System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { Session session = Session.create(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); final int MAX = 100; Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up, and expiry to happen if we are using expiring front cache Base.sleep(5000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 5 seconds to ensure JMX stats are up to date as well as expire entries second time around if the expiring cache is being used Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics Review the Tests The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 SimpleExpiringNearCachingExampleTest - uses all invalidation strategy and front expiry of 3 seconds There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. Review the SimpleExpiringNearCachingExampleTest <markup lang=\"java\" >public class SimpleExpiringNearCachingExampleTest { @Test public void testExpiringNearCache() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"expiring-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called expiring-cache-all , which matches the size limited and expiring near cache and invalidation strategy of all . Due to the expiry behaviour, the output will be different from the size limited cache. Run the Examples Run the examples using one of the methods below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest com.oracle.coherence.guides.nearcaching.SimpleExpiringNearCachingExampleTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations SimpleExpiringNearCachingExampleTest Output The output of this test is slightly different from the previous two as and expiring cache is used. See below for details of the output. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache expiring-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 21.364ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=expiring-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.2 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.543ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=expiring-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.2 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=expiring-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=110 &lt;Info&gt; (thread=main, member=1): Name: Size, value=10 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5238095238095238 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.2 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=expiring-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 21.364ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.543ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see the size is 10 as the total sleep time was &gt; 8 seconds which meant the near cache expired 100 entries before the next 10 entries were added Number of prune operations are 0 as no size limiting pruning has been done Because we are using the all invalidation strategy there is only 1 listener registered for all the entries Summary In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured expiry and eviction policies Explored MBeans related to near caching See Also Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "Near Caching"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Portable Types provide a way to add support for POF serialization to your classes via annotations and without the need to implement serialization code by hand, just like POF Annotations did. However, unlike POF Annotations, Portable Types: Implement serialization code at compile-time using byte code instrumentation, and do not rely on Java reflection at runtime at all. This makes them just as fast, but less error-prone, as manually implemented serialization code. Support, but do not require explicit registration via POF config file, as all the metadata required for POF type registration, such as type identifier, and the serializer class to use, are already available in the @PortableType annotation. Fully support class evolution. As a matter of fact, Portable Types provide a better and more complete evolution support than if you implemented Evolvable interface by hand. One of the limitations of Evolvable is that it only supports evolution of the leaf classes in the class hierarchy. Portable Types do not have this limitation, and allow you not only to evolve any class in the hierarchy, but also to evolve the class hierarchy itself, by adding new classes to any level of the class hierarchy. When we first introduced POF back in 2006, it was never the goal to require manual implementation of the serialization code&#8201;&#8212;&#8201;we always wanted to provide the tooling that would do the heavy lifting and allow users to simply express their intent via annotations. It may have taken us almost 15 years, but we feel that with the release of Portable Types, we are finally there. ",
            "title": "Features and Benefits"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " There are only two basic requirements for Portable Types: The class must be annotated with @PortableType annotation, and The fields that should be serialized must be annotated with @Portable or one of related annotations ( @PortableDate , @PortableArray , @PortableSet , @PortableList , or @PortableMap ) <markup lang=\"java\" >@PortableType(id = 1) public class Pet { @Portable protected String name; // constructors, accessors, etc. } @PortableType(id = 2) public class Dog extends Pet { @Portable private String breed; // constructors, accessors, etc. } Additional attribute-level annotations allow you to control certain serialization behaviors that are specific to the type of the attribute. For example, @PortableDate allows you to control whether you want to serialize date, time, or both when serializing java.util.Date instances (via mode property), and whether time zone information should be included (via includeTimezone property). If you are using Java 8 (or later) java.time classes, that information can be derived from the class itself, so you can (and should) simply use @Portable annotation instead. For example, LocalTime will be serialized as time only, with no time zone information, while the OffsetDateTime will be serialized as both date and time, with time zone information. Similarly, when serializing arrays, collections and maps, POF allows you to use uniform encoding , where the element type (or key and/or value type, in case of maps) is written into the POF stream only once, instead of once for each element of the collection, resulting in a more compact serialized form. <markup lang=\"java\" >public class MyClass { @PortableArray(elementClass = String.class) private String[] m_stringArray; @PortableSet(elementClass = String.class, clazz = LinkedHashSet.class) private Set&lt;String&gt; m_setOfStrings; @PortableList(elementClass = String.class) private List&lt;String&gt; m_listOfStrings; @PortableMap(keyClass = Integer.class, valueClass = String.class, clazz = TreeMap.class) private Map&lt;Integer, String&gt; m_uniformMap; } As you can see from the examples above, these annotations also allow you to specify the concrete class that should be created during deserialization for a given attribute. If the clazz property is not specified, HashSet will be used as a default set type, ArrayList as a default list type, and HashMap as a default map type. ",
            "title": "Usage Basics"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Coherence is a distributed system, and there is no guarantee that every cluster member, and every client process that connects to the cluster, will have the same version of each and every class. As a matter of fact, for systems that use rolling upgrades in order to avoid any downtime, it is pretty much guaranteed that they won&#8217;t! It is also neither safe nor practical for most Coherence customers to upgrade the cluster and all the clients at the same time, so being able to tolerate different versions of the same class across cluster members and clients is not only nice to have, but a necessity for many Coherence users. The issue is that when a process that has an older version of the class reads serialized data created from the newer version of the same class, it may encounter some attributes that it knows nothing about. Ideally, it should be able to ignore them and read the attributes it needs and knows about, instead of crashing, but that only solves part of the problem. If it ignores the unknown attributes completely, what will happen when it writes the same data back, by serializing an older version of the class that is only aware of some attributes? Unfortunately, the most likely answer is that it will lose the data it previously received but knows nothing about. Obviously, this is not a desirable scenario for a system that is intended for long-term data storage, so POF supports class evolution in a way that ensures that no data will be lost, regardless of how many versions of the same class are present across the various cluster and client processes, and regardless of which of those processes read or write the data. The support for class evolution has been in POF from the very beginning, via the Evolvable interface, but Portable Types remove some of the limitations and make the whole process significantly simpler. Both the class annotation ( @PortableType ) and the attribute annotations ( @Portable and related annotations) provide a way to specify versioning information necessary for class evolution. At the class level, whenever you modify a class by introducing a new attribute, you should increment the version property of the @PortableType annotation. At the same time, you should specify since attribute that matches the new class version number for any new class attribute. For example, to add age attribute to the Pet class, and color attribute to the Dog class, we would change the code above to: <markup lang=\"java\" >@PortableType(id = 1, version = 1) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; // constructors, accessors, etc. } @PortableType(id = 2, version = 1) public class Dog extends Pet { @Portable private String breed; @Portable(since = 1) private Color color; // constructors, accessors, etc. } Notice that both version and since properties are zero-based, which allows you to omit them completely in the initial implementation. It also means that for the first subsequent revision they should be set to 1 . Of course, those are just the defaults. You can certainly set the class and attribute version explicitly to any value even for the initial implementation, if you are so inclined. The only thing that matters is that you bump the version and set the since property to the latest version number whenever you make changes to the class in the future. For example, if in the future we decide to add height and weight attributes to the Pet class, we would simply increment the version to 2 and set the since property for the new attributes accordingly: <markup lang=\"java\" >@PortableType(id = 1, version = 2) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; @Portable(since = 2) protected int height; @Portable(since = 2) protected int weight; // constructors, accessors, etc. } Warning It may be obvious by now, but it&#8217;s probably worth calling out explicitly: class evolution allows you to add attributes to the new version of the class, but you should never remove existing attributes, as that will break serialization across class versions. You can certainly remove or deprecate attribute accessors from the class, but you should leave the field itself as-is, in order to preserve backwards compatibility of the serialized form. Along the same lines, you should avoid renaming the fields, as the default serialization order of fields is determined based on the alphabetical order of field names within a given class version (all fields with the same since value). ",
            "title": "Class Versioning and Evolution"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Annotating the classes is the first step in the implementation of Portable Types, but it is not sufficient on its own. In order to implement the necessary serialization logic, the classes also need to be instrumented at compile time. This is accomplished using the pof-maven-plugin , which should be configured in your POM file: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;20.12&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The configuration above will discover and instrument all project classes annotated with @PortableType annotation, including test classes. If you don&#8217;t need to instrument test classes you can omit the instrument-tests execution from the plugin configuration. The pof-maven-plugin uses Schema support to define the type system that contains all reachable portable types. This type system includes not only project classes that need to be instrumented, but also all portable types that exist in project dependencies. This is necessary because those dependent types may be used as attributes within the project classes, and need to be serialized appropriately. In some cases it may be necessary to expand the type system with the types that are not annotated with @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, assuming the Color class from the code examples above is an enum type, you would need to create the following META-INF/schema.xml file to register it and allow pof-maven-plugin to instrument Dog class correctly: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; Once all these bits and pieces are in place, you can simply run your build as usual: <markup lang=\"text\" >$ mvn clean install You can verify that the classes were instrumented successfully by checking the Maven output log. You should see something similar to the following: <markup lang=\"text\" >[INFO] --- pof-maven-plugin:20.12:instrument (instrument) @ petstore --- [INFO] Running PortableTypeGenerator for classes in /projects/petstore/target/classes [INFO] Instrumenting type petstore.Pet [INFO] Instrumenting type petstore.Dog Once the classes are successfully instrumented, they are ready to be registered and used. ",
            "title": "Compile-time Instrumentation"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Portable Object Format is not a self-describing serialization format: it replaces platform-specific class names with integer-based type identifiers , so it needs a way of mapping those type identifiers back to the platform-specific classes. This enables portability across platforms, which was, as the name clearly says, the main objective of POF. To manage the mappings between the type identifiers and concrete types, POF uses com.tangosol.io.pof.PofContext : <markup lang=\"java\" >public interface PofContext extends Serializer { PofSerializer getPofSerializer(int nTypeId); int getUserTypeIdentifier(Object o); int getUserTypeIdentifier(Class&lt;?&gt; clz); int getUserTypeIdentifier(String sClass); String getClassName(int nTypeId); Class&lt;?&gt; getClass(int nTypeId); boolean isUserType(Object o); boolean isUserType(Class&lt;?&gt; clz); boolean isUserType(String sClass); } It is worth noting that PofContext extends com.tangosol.io.Serializer interface, which means that any PofContext implementation can be used wherever Coherence expects a Serializer to be specified: within cache services as a storage-level serializer for data classes, as a transport-level serializer between thin clients and the proxy servers, etc. The PofContext performs the actual serialization by delegating to the appropriate PofSerializer , which is obtained via the PofContext.getPofSerializer method, based on a type identifier. There are several built-in implementations of PofContext . The SimplePofContext allows you to programmatically register type mappings by providing all the metadata needed for serialization, such as type identifier, class, and the PofSerializer to use: <markup lang=\"java\" >SimplePofContext ctx = new SimplePofContext(); ctx.registerUserType(1, Pet.class, new PortableTypeSerializer&lt;&gt;(1, Pet.class)); ctx.registerUserType(2, Dog.class, new PortableTypeSerializer&lt;&gt;(2, Dog.class)); ctx.registerUserType(3, Color.class, new EnumPofSerializer()); Notice that a lot of this information is somewhat repetitive and unnecessary when working with portable types, as all the metadata you need can be obtained from the class itself or the @PortableType annotation. Because of that, SimplePofContext also provides several convenience methods, specifically for portable types: <markup lang=\"java\" >ctx.registerPortableType(Pet.class); ctx.registerPortableType(Dog.class); Or even simpler: <markup lang=\"java\" >ctx.registerPortableTypes(Pet.class, Dog.class); While the SimplePofContext is useful for testing and quick prototyping, a PofContext implementation that is much more widely used within Coherence applications is ConfigurablePofContext . The ConfigurablePofContext allows you to provide type mappings via an external XML file: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;1&lt;/type-id&gt; &lt;class-name&gt;petstore.Pet&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;2&lt;/type-id&gt; &lt;class-name&gt;petstore.Dog&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;/pof-config&gt; You may notice that we didn&#8217;t have to specify serializer explicitly for Pet and Dog classes. This is because ConfigurablePofContext has the logic to determine which of the built-in PofSerializer implementations to use depending on the interfaces implemented by, or the annotations present on the specified class. In this case, it will automatically use PortableTypeSerializer because the classes have @PortableType annotation. However, we can make the configuration even simpler by enabling portable type discovery: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Once you set the enable-type-discovery flag to true , the ConfigurablePofContext will discover all the classes annotated with @PortableType and register them automatically, based on the annotation metadata. If we didn&#8217;t have the Color enum that has to be registered explicitly, we could even omit the configuration file completely, as the default pof-config.xml file that is built into Coherence looks like this: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;!-- by default just include coherence POF user types --&gt; &lt;include&gt;coherence-pof-config.xml&lt;/include&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Note The portable type discovery feature depends on the availability of a Jandex index within the modules that provide portable types that need to be registered. Make sure that you configure Jandex Maven Plugin to index classes in your modules at build time: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;process-classes&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; ",
            "title": "Registration and Discovery"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Once you have annotated, instrumented and registered portable types as described in the sections above, you can use them with Coherence just as easily as you would use plain Java Serializable classes, by configuring Coherence services to use pof serializer instead of the default java serializer. However, there is still one problem: serialization code is implemented by the pof-maven-plugin at compile-time, and only if you run Maven build, which can make it a bit cumbersome to run unit and integration tests within your IDE. In order to solve that problem, we have implemented IDE plugins for IntelliJ IDEA and Eclipse, which can instrument your classes during incremental or full compilation performed by your IDE. This allows you to test both the serialization of your classes and the code that depends on it without having to run Maven build or leave your IDE. Please follow the documentation for the Coherence IntelliJ Plugin or Coherence Eclipse Plugin for detailed instructions on how to install and use the plugin for your favorite IDE. Note We&#8217;ve used 1, 2, and 3 as type identifiers in the code and configuration examples above for simplicity, but it is worth noting that Coherence reserves type identifiers from 0 to 999 for internal use. That means that you should only use type identifiers of 1000 or higher for your own classes. ",
            "title": "IDE Support"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Portable Object Format (POF) was first introduced in Coherence 3.2 (2006), as a way to serialize classes in a platform and language independent format, and is the only serialization format supported by the legacy non-Java Extend clients, such as .NET and C++ Extend client implementations. As soon as it was released, POF became the preferred serialization format even for customers writing pure Java applications, for several reasons: It is significantly faster than other supported serialization formats, such as Java serialization and ExternalizableLite . It is significantly more compact that other supported serialization formats, allowing you to store more data in a cluster of a given size, and to move less data over the wire. It supports seamless evolution of data classes, allowing you to upgrade various parts of the application (both storage members and clients) independently of one another, without the risk of losing data in the process. Over the years POF remained largely unchanged, even though it did receive a number of additional features that simplified its use: POF Reflection was introduced in Coherence 3.5 (2009), allowing users to extract individual attributes from the POF stream via PofNavigator . POF Annotations were introduced in Coherence 3.7.1 (2011), as a way to eliminate the need for the manual implementation of the serialization-related code. Unfortunately, the latter fell a bit short. The implementation was heavily dependent on Java reflection, which sacrificed some performance benefits of POF. More importantly, they provide no support for class evolution, thus sacrificing another important POF benefit. As such, POF Annotations were deemed somewhat inadequate, and we started working on their replacement in 2013. Some supporting features, such as schema support , were included in Coherence 12.2.1 (2015) and 14.1.1 (2020), and the remaining work was completed and released as part of the Coherence CE 20.12 release and will be available in the next commercial release. Features and Benefits Portable Types provide a way to add support for POF serialization to your classes via annotations and without the need to implement serialization code by hand, just like POF Annotations did. However, unlike POF Annotations, Portable Types: Implement serialization code at compile-time using byte code instrumentation, and do not rely on Java reflection at runtime at all. This makes them just as fast, but less error-prone, as manually implemented serialization code. Support, but do not require explicit registration via POF config file, as all the metadata required for POF type registration, such as type identifier, and the serializer class to use, are already available in the @PortableType annotation. Fully support class evolution. As a matter of fact, Portable Types provide a better and more complete evolution support than if you implemented Evolvable interface by hand. One of the limitations of Evolvable is that it only supports evolution of the leaf classes in the class hierarchy. Portable Types do not have this limitation, and allow you not only to evolve any class in the hierarchy, but also to evolve the class hierarchy itself, by adding new classes to any level of the class hierarchy. When we first introduced POF back in 2006, it was never the goal to require manual implementation of the serialization code&#8201;&#8212;&#8201;we always wanted to provide the tooling that would do the heavy lifting and allow users to simply express their intent via annotations. It may have taken us almost 15 years, but we feel that with the release of Portable Types, we are finally there. Usage Basics There are only two basic requirements for Portable Types: The class must be annotated with @PortableType annotation, and The fields that should be serialized must be annotated with @Portable or one of related annotations ( @PortableDate , @PortableArray , @PortableSet , @PortableList , or @PortableMap ) <markup lang=\"java\" >@PortableType(id = 1) public class Pet { @Portable protected String name; // constructors, accessors, etc. } @PortableType(id = 2) public class Dog extends Pet { @Portable private String breed; // constructors, accessors, etc. } Additional attribute-level annotations allow you to control certain serialization behaviors that are specific to the type of the attribute. For example, @PortableDate allows you to control whether you want to serialize date, time, or both when serializing java.util.Date instances (via mode property), and whether time zone information should be included (via includeTimezone property). If you are using Java 8 (or later) java.time classes, that information can be derived from the class itself, so you can (and should) simply use @Portable annotation instead. For example, LocalTime will be serialized as time only, with no time zone information, while the OffsetDateTime will be serialized as both date and time, with time zone information. Similarly, when serializing arrays, collections and maps, POF allows you to use uniform encoding , where the element type (or key and/or value type, in case of maps) is written into the POF stream only once, instead of once for each element of the collection, resulting in a more compact serialized form. <markup lang=\"java\" >public class MyClass { @PortableArray(elementClass = String.class) private String[] m_stringArray; @PortableSet(elementClass = String.class, clazz = LinkedHashSet.class) private Set&lt;String&gt; m_setOfStrings; @PortableList(elementClass = String.class) private List&lt;String&gt; m_listOfStrings; @PortableMap(keyClass = Integer.class, valueClass = String.class, clazz = TreeMap.class) private Map&lt;Integer, String&gt; m_uniformMap; } As you can see from the examples above, these annotations also allow you to specify the concrete class that should be created during deserialization for a given attribute. If the clazz property is not specified, HashSet will be used as a default set type, ArrayList as a default list type, and HashMap as a default map type. Class Versioning and Evolution Coherence is a distributed system, and there is no guarantee that every cluster member, and every client process that connects to the cluster, will have the same version of each and every class. As a matter of fact, for systems that use rolling upgrades in order to avoid any downtime, it is pretty much guaranteed that they won&#8217;t! It is also neither safe nor practical for most Coherence customers to upgrade the cluster and all the clients at the same time, so being able to tolerate different versions of the same class across cluster members and clients is not only nice to have, but a necessity for many Coherence users. The issue is that when a process that has an older version of the class reads serialized data created from the newer version of the same class, it may encounter some attributes that it knows nothing about. Ideally, it should be able to ignore them and read the attributes it needs and knows about, instead of crashing, but that only solves part of the problem. If it ignores the unknown attributes completely, what will happen when it writes the same data back, by serializing an older version of the class that is only aware of some attributes? Unfortunately, the most likely answer is that it will lose the data it previously received but knows nothing about. Obviously, this is not a desirable scenario for a system that is intended for long-term data storage, so POF supports class evolution in a way that ensures that no data will be lost, regardless of how many versions of the same class are present across the various cluster and client processes, and regardless of which of those processes read or write the data. The support for class evolution has been in POF from the very beginning, via the Evolvable interface, but Portable Types remove some of the limitations and make the whole process significantly simpler. Both the class annotation ( @PortableType ) and the attribute annotations ( @Portable and related annotations) provide a way to specify versioning information necessary for class evolution. At the class level, whenever you modify a class by introducing a new attribute, you should increment the version property of the @PortableType annotation. At the same time, you should specify since attribute that matches the new class version number for any new class attribute. For example, to add age attribute to the Pet class, and color attribute to the Dog class, we would change the code above to: <markup lang=\"java\" >@PortableType(id = 1, version = 1) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; // constructors, accessors, etc. } @PortableType(id = 2, version = 1) public class Dog extends Pet { @Portable private String breed; @Portable(since = 1) private Color color; // constructors, accessors, etc. } Notice that both version and since properties are zero-based, which allows you to omit them completely in the initial implementation. It also means that for the first subsequent revision they should be set to 1 . Of course, those are just the defaults. You can certainly set the class and attribute version explicitly to any value even for the initial implementation, if you are so inclined. The only thing that matters is that you bump the version and set the since property to the latest version number whenever you make changes to the class in the future. For example, if in the future we decide to add height and weight attributes to the Pet class, we would simply increment the version to 2 and set the since property for the new attributes accordingly: <markup lang=\"java\" >@PortableType(id = 1, version = 2) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; @Portable(since = 2) protected int height; @Portable(since = 2) protected int weight; // constructors, accessors, etc. } Warning It may be obvious by now, but it&#8217;s probably worth calling out explicitly: class evolution allows you to add attributes to the new version of the class, but you should never remove existing attributes, as that will break serialization across class versions. You can certainly remove or deprecate attribute accessors from the class, but you should leave the field itself as-is, in order to preserve backwards compatibility of the serialized form. Along the same lines, you should avoid renaming the fields, as the default serialization order of fields is determined based on the alphabetical order of field names within a given class version (all fields with the same since value). Compile-time Instrumentation Annotating the classes is the first step in the implementation of Portable Types, but it is not sufficient on its own. In order to implement the necessary serialization logic, the classes also need to be instrumented at compile time. This is accomplished using the pof-maven-plugin , which should be configured in your POM file: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;20.12&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The configuration above will discover and instrument all project classes annotated with @PortableType annotation, including test classes. If you don&#8217;t need to instrument test classes you can omit the instrument-tests execution from the plugin configuration. The pof-maven-plugin uses Schema support to define the type system that contains all reachable portable types. This type system includes not only project classes that need to be instrumented, but also all portable types that exist in project dependencies. This is necessary because those dependent types may be used as attributes within the project classes, and need to be serialized appropriately. In some cases it may be necessary to expand the type system with the types that are not annotated with @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, assuming the Color class from the code examples above is an enum type, you would need to create the following META-INF/schema.xml file to register it and allow pof-maven-plugin to instrument Dog class correctly: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; Once all these bits and pieces are in place, you can simply run your build as usual: <markup lang=\"text\" >$ mvn clean install You can verify that the classes were instrumented successfully by checking the Maven output log. You should see something similar to the following: <markup lang=\"text\" >[INFO] --- pof-maven-plugin:20.12:instrument (instrument) @ petstore --- [INFO] Running PortableTypeGenerator for classes in /projects/petstore/target/classes [INFO] Instrumenting type petstore.Pet [INFO] Instrumenting type petstore.Dog Once the classes are successfully instrumented, they are ready to be registered and used. Registration and Discovery Portable Object Format is not a self-describing serialization format: it replaces platform-specific class names with integer-based type identifiers , so it needs a way of mapping those type identifiers back to the platform-specific classes. This enables portability across platforms, which was, as the name clearly says, the main objective of POF. To manage the mappings between the type identifiers and concrete types, POF uses com.tangosol.io.pof.PofContext : <markup lang=\"java\" >public interface PofContext extends Serializer { PofSerializer getPofSerializer(int nTypeId); int getUserTypeIdentifier(Object o); int getUserTypeIdentifier(Class&lt;?&gt; clz); int getUserTypeIdentifier(String sClass); String getClassName(int nTypeId); Class&lt;?&gt; getClass(int nTypeId); boolean isUserType(Object o); boolean isUserType(Class&lt;?&gt; clz); boolean isUserType(String sClass); } It is worth noting that PofContext extends com.tangosol.io.Serializer interface, which means that any PofContext implementation can be used wherever Coherence expects a Serializer to be specified: within cache services as a storage-level serializer for data classes, as a transport-level serializer between thin clients and the proxy servers, etc. The PofContext performs the actual serialization by delegating to the appropriate PofSerializer , which is obtained via the PofContext.getPofSerializer method, based on a type identifier. There are several built-in implementations of PofContext . The SimplePofContext allows you to programmatically register type mappings by providing all the metadata needed for serialization, such as type identifier, class, and the PofSerializer to use: <markup lang=\"java\" >SimplePofContext ctx = new SimplePofContext(); ctx.registerUserType(1, Pet.class, new PortableTypeSerializer&lt;&gt;(1, Pet.class)); ctx.registerUserType(2, Dog.class, new PortableTypeSerializer&lt;&gt;(2, Dog.class)); ctx.registerUserType(3, Color.class, new EnumPofSerializer()); Notice that a lot of this information is somewhat repetitive and unnecessary when working with portable types, as all the metadata you need can be obtained from the class itself or the @PortableType annotation. Because of that, SimplePofContext also provides several convenience methods, specifically for portable types: <markup lang=\"java\" >ctx.registerPortableType(Pet.class); ctx.registerPortableType(Dog.class); Or even simpler: <markup lang=\"java\" >ctx.registerPortableTypes(Pet.class, Dog.class); While the SimplePofContext is useful for testing and quick prototyping, a PofContext implementation that is much more widely used within Coherence applications is ConfigurablePofContext . The ConfigurablePofContext allows you to provide type mappings via an external XML file: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;1&lt;/type-id&gt; &lt;class-name&gt;petstore.Pet&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;2&lt;/type-id&gt; &lt;class-name&gt;petstore.Dog&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;/pof-config&gt; You may notice that we didn&#8217;t have to specify serializer explicitly for Pet and Dog classes. This is because ConfigurablePofContext has the logic to determine which of the built-in PofSerializer implementations to use depending on the interfaces implemented by, or the annotations present on the specified class. In this case, it will automatically use PortableTypeSerializer because the classes have @PortableType annotation. However, we can make the configuration even simpler by enabling portable type discovery: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Once you set the enable-type-discovery flag to true , the ConfigurablePofContext will discover all the classes annotated with @PortableType and register them automatically, based on the annotation metadata. If we didn&#8217;t have the Color enum that has to be registered explicitly, we could even omit the configuration file completely, as the default pof-config.xml file that is built into Coherence looks like this: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;!-- by default just include coherence POF user types --&gt; &lt;include&gt;coherence-pof-config.xml&lt;/include&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Note The portable type discovery feature depends on the availability of a Jandex index within the modules that provide portable types that need to be registered. Make sure that you configure Jandex Maven Plugin to index classes in your modules at build time: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;process-classes&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; IDE Support Once you have annotated, instrumented and registered portable types as described in the sections above, you can use them with Coherence just as easily as you would use plain Java Serializable classes, by configuring Coherence services to use pof serializer instead of the default java serializer. However, there is still one problem: serialization code is implemented by the pof-maven-plugin at compile-time, and only if you run Maven build, which can make it a bit cumbersome to run unit and integration tests within your IDE. In order to solve that problem, we have implemented IDE plugins for IntelliJ IDEA and Eclipse, which can instrument your classes during incremental or full compilation performed by your IDE. This allows you to test both the serialization of your classes and the code that depends on it without having to run Maven build or leave your IDE. Please follow the documentation for the Coherence IntelliJ Plugin or Coherence Eclipse Plugin for detailed instructions on how to install and use the plugin for your favorite IDE. Note We&#8217;ve used 1, 2, and 3 as type identifiers in the code and configuration examples above for simplicity, but it is worth noting that Coherence reserves type identifiers from 0 to 999 for internal use. That means that you should only use type identifiers of 1000 or higher for your own classes. ",
            "title": "Portable Types"
        },
        {
            "location": "/docs/README",
            "text": " To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs ",
            "title": "Build the Docs"
        },
        {
            "location": "/docs/README",
            "text": " To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "View the Docs"
        },
        {
            "location": "/docs/README",
            "text": " This is the module that builds the Coherence documentation. The module is not part of the default build and must be built separately. Build the Docs To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs View the Docs To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "Coherence Documentation Module"
        },
        {
            "location": "/docs/README",
            "text": " When putting version numbers in .adoc files, we use attribute substitutions. Attributes are set in the sitegen.yaml file, for example <markup lang=\"yaml\" >engine: asciidoctor: images-dir: \"docs/images\" libraries: - \"asciidoctor-diagram\" attributes: plantumlconfig: \"_plantuml-config.txt\" coherence-maven-group-id: \"${coherence.group.id}\" version-coherence: \"${revision}\" version-commercial-docs: \"14.1.1.0\" version-helidon: \"${helidon.version}\" The format of an attribute is name followed by a colon, and the attribute value in quotes, so above the value of the version-commercial-docs attribute is 14.1.1.0 . Attributes can be taken from Maven build properties by using the normal Maven property replacement string as the value. For example the version-coherence attribute&#8217;s value will be the Maven revision property value. In the .adoc files the attributes are then substituted by putting the attribute name in curly brackets. For example: The current commercial Coherence version is 14.1.1.0. would become The current commercial Coherence version is 14.1.1.0. ",
            "title": "Version Numbers"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " First and foremost, Coherence provides a fundamental service that is responsible for all facets of clustering and is a common denominator / building block for all other Coherence services. This service, referred to as 'service 0' internally, ensures the mesh of members is maintained and responsive, taking action to collaboratively evict, shun, or in some cases voluntarily depart the cluster when deemed necessary. As members join and leave the cluster, other Coherence services are notified thus allows those services to react accordingly. This part of the Coherence product has been in production for 10+ years, being the subject of some extensive and imaginative testing. While it has been discussed here it certainly is not something that customers, generally, interact with directly but is valuable to be aware of. Coherence services build on top of the clustering service, with the key implementations to be aware of are PartitionedService, InvocationService, and ProxyService. In the majority of cases customers will deal with caches; a cache will be represented by an implementation of NamedCache&lt;K,V&gt; . Cache is an unfortunate name, as many Coherence customers use Coherence as a system-of-record rather than a lossy store of data. A cache is hosted by a service, generally the PartitionedService, and is the entry point to storing, retrieving, aggregating, querying, and streaming data. There are a number of features that caches provide: Fundamental key-based access : get/put getAll/putAll Client-side and storage-side events MapListeners to asynchronously notify clients of changes to data EventInterceptors (either sync or async) to be notified storage level events, including mutations, partition transfer, failover, etc NearCaches - locally cached data based on previous requests with local content invalidated upon changes in storage tier ViewCaches - locally stored view of remote data that can be a subset based on a predicate and is kept in sync real time Queries - distributed, parallel query evaluation to return matching key, values or entries with potential to optimize performance with indices Aggregations - a map/reduce style aggregation where data is aggregated in parallel on all storage nodes and results streamed back to the client for aggregation of those results to produce a final result Data local processing - an ability to send a function to the relevant storage node to execute processing logic for the appropriate entries with exclusive access Partition local transactions - an ability to perform scalable transactions by associating data (thus being on the same partition) and manipulating other entries on the same partition potentially across caches Non-blocking / async NamedCache API C&#43;&#43; and .NET clients - access the same NamedCache API from either C&#43;&#43; or .NET Portable Object Format - optimized serialization format, with the ability to navigate the serialized form for optimized queries, aggregations, or data processing Integration with Databases - Database &amp; third party data integration with CacheStores including both synchronous or asynchronous writes CohQL - ansi-style query language with a console for adhoc queries Topics - distributed topics implementation offering pub/sub messaging with the storage capacity the cluster and parallelizable subscribers There are also a number of non-functional features that Coherence provides: Rock solid clustering - highly tuned and robust clustering stack that allows Coherence to scale to thousands of members in a cluster with thousands of partitions and terabytes of data being accessed, mutated, queried and aggregated concurrently Safety first - resilient data management that ensures backup copies are on distinct machines, racks, or sites and the ability to maintain multiple backups 24/7 Availability - zero down time with rolling redeploy of cluster members to upgrade application or product versions Backwards and forwards compatibility of product upgrades, including major versions Persistent Caches - with the ability to use local file system persistence (thus avoid extra network hops) and leverage Coherence consensus protocols to perform distributed disk recovery when appropriate Distributed State Snapshot - ability to perform distributed point-in-time snapshot of cluster state, and recover snapshot in this or a different cluster (leverages persistence feature) Lossy redundancy - ability to reduce the redundancy guarantee by making backups and/or persistence asynchronous from a client perspective Single Mangement View - provides insight into the cluster with a single JMX server that provides a view of all members of the cluster Management over REST - all JMX data and operations can be performed over REST, including cluster wide thread dumps and heapdumps Non-cluster Access - access to the cluster from the outside via proxies, for distant (high latency) clients and for non-java languages such as C&#43;&#43; and .NET Kubernetes friendly - seamlessly and safely deploy applications to k8s with our own operator ",
            "title": "Introduction"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Coherence Community Edition does not include the following Oracle Coherence commercial edition functionality Management of Coherence via the Oracle WebLogic Management Framework WebLogic Server Multi-tenancy support Deployment of Grid Archives (GARs) HTTP session management for application servers (Coherence*Web) GoldenGate HotCache TopLink-based CacheLoaders and CacheStores Elastic Data Federation and WAN (wide area network) support Transaction Framework CommonJ work manager ",
            "title": "Coherence Community Edition Disabled and Excluded Functionality"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Java - jdk8 or higher Maven - 3.6.3 or higher ",
            "title": "Prerequisites"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "How to Get Coherence Community Edition"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Prerequisites Java - jdk8 or higher Maven - 3.6.3 or higher How to Get Coherence Community Edition As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "Quick Start"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.06-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.06-SNAPSHOT/coherence-21.06-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 ",
            "title": " CohQL Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.06-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.06-SNAPSHOT/coherence-21.06-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": " Coherence Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates the procedure to start a storage enabled Coherence Server, followed by a storage disabled Coherence Console. Using the console, data is inserted, retrieved, and then the console is terminated. The console is restarted and data is once again retrieved to illustrate the permanence of the data. This example uses the out-of-the-box cache configuration and therefore explicitly specifying the console is storage disabled is unnecessary. Coherence cluster members discover each other via one of two mechanisms; multicast (default) or Well Known Addressing (deterministic broadcast). If your system does not support multicast, enable WKA by specifying -Dcoherence.wka=localhost for both processes started in the following console examples. CohQL Console To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.06-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.06-SNAPSHOT/coherence-21.06-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 Coherence Console To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.06-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.06-SNAPSHOT/coherence-21.06-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": "CLI Hello Coherence"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": "Build HelloCoherence "
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates starting a storage enabled Coherence server, followed by running the HelloCoherence application. The HelloCoherence application inserts and retrieves data from the Coherence server. Build HelloCoherence Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": " Programmatic Hello Coherence Example"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": "<markup lang=\"shell\" >$&gt; git clone git@github.com:oracle/coherence.git $&gt; cd coherence/prj # build all modules $&gt; mvn clean install # build all modules skipping tests $&gt; mvn clean install -DskipTests # build a specific module, including all dependent modules and run tests $&gt; mvn -am -pl test/functional/persistence clean verify # build only coherence.jar without running tests $&gt; mvn -am -pl coherence clean install -DskipTests # build only coherence.jar and skip compilation of CDBs and tests $&gt; mvn -am -pl coherence clean install -DskipTests -Dtde.compile.not.required ",
            "title": " Building"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI provides support for CDI (Contexts and Dependency Injection) within Coherence cluster members. It allows you both to inject Coherence-managed resources, such as NamedMap , NamedCache and Session instances into CDI managed beans, to inject CDI beans into Coherence-managed resources, such as event interceptors and cache stores, and to handle Coherence server-side events using CDI observer methods. In addition, Coherence CDI provides support for automatic injection of transient objects upon deserialization. This allows you to inject CDI managed beans such as services and repositories (to use DDD nomenclature) into transient objects, such as entry processor and even data class instances, greatly simplifying implementation of true Domain Driven applications. ",
            "title": "Coherence CDI"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Inject Views"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import javax.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject @Name(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Injecting NamedMap , NamedCache and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"payments-cluster.xml\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import javax.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; ",
            "title": "Injecting NamedTopic and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; ",
            "title": " Cluster and OperationalContext Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; ",
            "title": "Named Session Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; ",
            "title": " Serializer Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Inject a POF Serializer With a Specific POF Configuration"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Other Supported Injection Points"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import javax.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject @Name(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"payments-cluster.xml\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import javax.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } ",
            "title": "Observe Events for Maps and Caches in Specific Services and Scopes"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using CDI Observers to Handle Coherence Server-Side Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Injecting CDI Beans into Coherence-managed Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Making transient classes Injectable "
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Injecting CDI Beans into Transient Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . ",
            "title": "Create the Custom Filter Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. ",
            "title": "Create the Custom Filter Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "FilterBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ",
            "title": "PropertyExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); ",
            "title": "ChainedExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "PofExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "Built-In ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. ",
            "title": "Custom ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "MapEventTransformerBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to use Coherence CDI, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using CDI to inject Coherence objects into managed CDI beans, and vice versa, as the following sections describe. Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache`, and related objects Injecting NamedMap or NamedCache Views Injecting NamedTopic and related objects Other Supported Injection Points Cluster and OperationalContext Injection Named Session Injection Serializer Injection Injecting CDI Beans into Coherence-managed Objects Using CDI Observers to Handle Coherence Server-Side Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Injecting CDI Beans into Transient Objects Making transient classes Injectable Filter Binding Annotations Extractor Binding Annotations Built-In Extractor Binding Annotations @PropertyExtractor @ChainedExtractor @PofExtractor Custom Extractor Binding Annotations MapEventTransformer Binding Annotations Injecting Coherence Objects into CDI Beans CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import javax.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject @Name(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"payments-cluster.xml\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import javax.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. Injecting CDI Beans into Coherence-managed Objects Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. Injecting CDI Beans into Transient Objects Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. FilterBinding Annotations As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ExtractorBinding Annotations Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { MapEventTransformerBinding Annotations Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Usage"
        },
        {
            "location": "/docs/core/01_overview",
            "text": " Coherence has a number of core improvements documented in this section. fa-rocket Bootstrap API Bootstrap Coherence application. library_books Parallel Recovery Recover data from Coherence&#8217;s Persistence mechanism in parallel. fa-sitemap Portable Types Implement versioned data classes that can evolve over time. fa-sitemap Repository API Higher level, DDD-friendly data access API. ",
            "title": "Overview"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " In addition to the basic CRUD functionality, the Repository API provides many features that simplify common data management tasks: Powerful projection features Flexible in-place entity updates First-class data aggregation support Stream API support Event listener support Declarative acceleration and index creation CDI Support ",
            "title": "Features and Benefits"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Coherence provides an abstract base class com.oracle.coherence.repository.AbstractRepository , which your custom repository implementation needs to extend and provide implementation of the three abstract methods: <markup lang=\"java\" > /** * Return the {@link NamedMap} that is used as the underlying entity store. * * @return the {@link NamedMap} that is used as the underlying entity store */ protected abstract NamedMap&lt;ID, T&gt; getMap(); /** * Return the identifier of the specified entity instance. * * @param entity the entity to get the identifier from * * @return the identifier of the specified entity instance */ protected abstract ID getId(T entity); /** * Return the type of entities in this repository. * * @return the type of entities in this repository */ protected abstract Class&lt;? extends T&gt; getEntityType(); For example, a repository implementation that can be used to store Person entities, with String identifiers, can be as simple as: <markup lang=\"java\" >public class PeopleRepository extends AbstractRepository&lt;String, Person&gt; { private NamedMap&lt;String, Person&gt; people; public PeopleRepository(NamedMap&lt;String, Person&gt; people) { this.people = people; } protected NamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person person) { return person.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the NamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository That is it in a nutshell: a trivial repository implementation above will allow you to access all the Repository API features described in the remaining sections, which are provided by the AbstractRepository class you extended. However, you are free (and encouraged) to add additional business methods to the repository class above that will make it easier to use within your application. The most common example of such methods would be various \"finder\" methods that your application needs. For example, if your application needs to frequently query the repository to find people based on their name, you may want to add a method for that purpose: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return findAll(filter); } You can then invoke findByName method directly within the application to find all the people whose first or last name starts with a letter A , for example: <markup lang=\"java\" >for (Person p : people.findByName(\"A%\")) { // processing } ",
            "title": "Implementing a Repository"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " We&#8217;ve already seen one read operation, findAll , in the example above, but let&#8217;s start from the beginning and look into how we can add, remove, update and query our repository. To add new entities to the repository, or replace the existing ones, you can use either the save or the saveAll method. The former takes a single entity as an argument and stores it in the backing NamedMap : <markup lang=\"java\" >people.save(new Person(\"555-11-2222\", \"Aleks\", 46)); The latter allows you to store a batch of entities at once by passing either an array, a collection or a stream of entities as an argument. Once you have some entities stored in a repository, you can query the repository using findById and findAll methods. <markup lang=\"java\" >Person person = people.findById(\"555-11-2222\"); assert person.getName().equals(\"Aleks\"); assert person.getAge() == 46; Collection&lt;Person&gt; allPeople = people.findAll(); Collection&lt;Person&gt; allAdults = people.findAll(Filters.greaterOrEqual(Person::getAge, 18)); find a single Person by identifier find all the people in the repository find all the people in the repository that are 18 or older When using findAll , you can also specify an optional sort order for the results, by specifying a Comparable property via a method reference: <markup lang=\"java\" >Collection&lt;Person&gt; peopleOrderedByAge = people.findAll(Person::getAge) the result will contain all people from the repository, sorted by age from the youngest to the oldest For more complex use cases, you can specify a Comparator to use instead. For example, if we wanted to always sort the results of the findByName method defined above first by last name and then by first name, we could re-implement it as: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return findAll(filter, Remote.comparator(Person::getLastName) .thenComparing(Person::getFirstName)); } the results will be sorted by last name, and then by first name; note that we are using Coherence Remote.comparator instead of standard Java Comparator in order to ensure that the specified comparator is serializable and can be sent to remote cluster members. Finally, to remove entities from a repository you can use one of several remove methods: <markup lang=\"java\" >boolean fRemoved = people.remove(person); boolean fRemoved = people.removeById(\"111-22-3333\"); removes specified entity from the repository removes entity with the specified identifier from the repository In both examples above the result will be a boolean indicating whether the entity was actually removed from the backing NamedMap , and it may be false if the entity wasn&#8217;t present in the repository. If you are interested in the removed value itself, you can use the overloads of the methods above that allow you to express that: <markup lang=\"java\" >Person removed = people.remove(person, true); Person removed = people.removeById(\"111-22-3333\", true); removes specified entity from the repository and returns it as the result removes entity with the specified identifier from the repository and returns it as the result Note that this will result in additional network traffic, so unless you really need the removed entity it is probably best not to ask for it. The examples above are useful when you want to remove a single entity from the repository. In cases when you want to remove multiple entities as part of a single network call, you should use one of removeAll methods instead, which allow you to remove a set of entities by specifying either their identifiers explicitly, or the criteria for removal via the Filter . <markup lang=\"java\" >boolean fChanged = people.removeAll(Filters.equal(Person::getGender, Gender.MALE)); boolean fChanged = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\")); removes all men from the repository and returns true if any entity has been removed removes entities with the specified identifiers from the repository and returns true if any entity has been removed Just like with single-entity removal operations, you can also use overloads that allow you to return the removed entities as the result: <markup lang=\"java\" >Map&lt;String, Person&gt; mapRemoved = people.removeAll(Filters.equal(Person::getGender, Gender.MALE), true); Map&lt;String, Person&gt; mapRemoved = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\"), true); removes all men from the repository and returns the map of removed entities, keyed by identifier removes entities with the specified identifiers from the repository and returns the map of removed entities, keyed by identifier ",
            "title": "Basic CRUD Operations"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " While querying repository for a collection of entities that satisfy some criteria is certainly a common and useful operation, sometimes you don&#8217;t need all the information contained within the entity. For example, if you only need a person&#8217;s name, querying for and then discarding all the information contained within the Person instances is unnecessary and wasteful. It is the equivalent of executing <markup lang=\"sql\" >SELECT * FROM PEOPLE against a relational database, when a simple <markup lang=\"sql\" >SELECT name FROM PEOPLE would suffice. Coherence Repository API allows you to limit the amount of data collected by performing server-side projection of the entity attributes you are interested in. For example, if you only need a person&#8217;s name, you can get just the name: <markup lang=\"java\" >String name = people.get(\"111-22-3333\", Person::getName); Map&lt;String, String&gt; mapNames = people.getAll(Filters.less(Person::getAge, 18), Person::getName); return the name of the person with a specified identifier return the map of names of all the people younger than 18, keyed by person&#8217;s identifier Obviously, returning either the whole entity or a single attribute from an entity are two ends of the spectrum, and more often than not you need something in between. For example, you may need the person&#8217;s name and age. For situations like that, Coherence allows you to use fragments : <markup lang=\"java\" >Fragment&lt;Person&gt; fragment = people.get(\"111-22-3333\", Person::getName, Person::getAge); String name = fragment.get(Person::getName); int age = fragment.get(Person::getAge); return a fragment containing the name and age of the person with a specified identifier retrieve the person&#8217;s name from a fragment retrieve the person&#8217;s age from a fragment You can, of course, perform the same projection across multiple entities using one of getAll methods: <markup lang=\"java\" >Map&lt;String, Fragment&lt;Person&gt;&gt; fragments = people.getAll( Filters.less(Person::getAge, 18), Person::getName, Person::getAge); return a map of fragments containing the name and age of all the people younger than 18, keyed by person&#8217;s identifier Unlike the relational database, which contains a set of columns for each row in the table, Coherence stores each entity as a full object graph, which means that the attributes can be other object graphs and can be nested to any level. This means that we also may need to be able to project attributes of the nested objects. For example, our Person class may have a nested Address object as an attribute, which in turn has street , city , and country attributes. If we want to retrieve the name and the country of a person in a repository, we can do it like this: <markup lang=\"java\" >Fragment&lt;Person&gt; person = people.get( \"111-22-3333\", Person::getName, Extractors.fragment(Person::getAddress, Address::getCountry)); String name = person.get(Person::getName); Fragment&lt;Address&gt; address = person.getFragment(Person::getAddress); String country = address.get(Address::getCountry); return a fragment containing the name and the Address fragment of the person with a specified identifier retrieve the person&#8217;s name from the Person fragment retrieve the Address fragment from the Person fragment retrieve the person&#8217;s country from the Address fragment ",
            "title": "Projection"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " By far the most common approach for updating data in modern applications is the read-modify-write pattern. For example, the typical code to update an attribute of a Person may look similar to the following: <markup lang=\"java\" >Person person = people.findById(\"111-22-3333\"); person.setAge(55); people.save(person); This is true regardless of whether the underlying data store provides a better, more efficient way of updating data. For example, RDBMS provide stored procedures for that purpose, but very few developers use them because they are not as convenient to use, and do not fit well into popular application frameworks, such as JPA, Spring Data or Micronaut Data. They also fragment the code base to some extent, splitting the business logic across the application and the data store, and require that some application code is written in SQL. However, the approach above is suboptimal, for a number of reasons: It at least doubles the number of network calls the application makes to the data store, increasing the overall latency of the operation. It moves (potentially a lot) more data over the network than absolutely necessary. It may require expensive construction of a complex entity in order to perform a very simple update operation of a single attribute (this is particularly true with JPA and RDBMS back ends). It puts additional, unnecessary load on the data store, which is typically the hardest component of the application to scale. It introduces concurrency issues (ie. what should happen if the entity in the data store changes between the initial read and subsequent write), which typically requires that both the read and the write happen within the same transaction. A much better, more efficient way to perform the updates is to send the update function to the data store, and execute it locally, within the data store itself (which is pretty much what stored procedures are for). Coherence has always had support for these types of updates via entry processors , but the Repository API makes it even simpler to do so. For example, the code above can be rewritten as: <markup lang=\"java\" >people.update(\"111-22-3333\", Person::setAge, 55); We are basically telling Coherence to update Person instance with a given identifier by calling setAge method on it with a number 55 as an argument. This is not only significantly more efficient, but I&#8217;m sure you&#8217;ll agree, shorter and easier to write, and to read. Note that we don&#8217;t know, or care, where in the cluster a Person instance with a given identifier is&#8201;&#8212;&#8201;all we care about is that Coherence guarantees that it will invoke the setAge method on the entity with a specified ID, on a primary owner , and automatically create a backup of the modified entity for fault tolerance. It is also worth pointing out that the approach above provides the same benefits stored procedures do in RDBMS, but without the downsides: you are still writing all your code in Java, and keeping it in the same place. As a matter of fact, this approach allows you to implement rich domain models for your data, and execute business logic on your entities remotely, which works exceptionally well with DDD applications. Calling a setter on an entity remotely is only the tip of the iceberg, and far from sufficient for all data mutation needs. For example, conventional JavaBean setter returns void , but you often want to know what the entity value is after the update. The solution to that problem is simple: Coherence will return the result of the specified method invocation, so all you need to do is change the setAge method to implement fluent API: <markup lang=\"java\" >public Person setAge(int age) { this.age = age; return this; } You will now get the modified Person instance as the result of the update call: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", Person::setAge, 55); assert person.getAge() == 55; Sometimes you need to perform more complex updates, or update multiple attributes at the same time. While you could certainly accomplish both of those by making multiple update calls, that is inefficient because each update will result in a separate network call. You are better off using the update overload that allows you to specify the function to execute in that situation: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", p -&gt; { p.setAge(55); p.setGender(Gender.MALE); return p; }); assert person.getAge() == 55; assert person.getGender() == Gender.MALE; This way you have full control of the update logic that will be executed, and the return value. You may sometimes want to update an entity that does not exist in the repository yet, in which case you want to create a new instance. For example, you may want to create a shopping cart entity for a customer when they add the first item to the cart. While you could implement the code to check whether the Cart for a given customer exists, and create new one if it doesn&#8217;t, this again results in network calls that can be avoided if you simply create the Cart instance as part of Cart::addItem call. The Repository API allows you to accomplish that via optional EntityFactory argument: <markup lang=\"java\" >carts.update(customerId, Cart::addItem, item, Cart::new); the cart/customer identifier the method to invoke on a target Cart instance the CartItem to add to the cart the EntityFactory to use to create a new Cart instance if the cart with the specified identifier doesn&#8217;t exist The EntityFactory interface is quite simple: <markup lang=\"java\" >@FunctionalInterface public interface EntityFactory&lt;ID, T&gt; extends Serializable { /** * Create an entity instance with the specified identity. * * @param id identifier to create entity instance with * * @return a created entity instance */ T create(ID id); } Basically, it has a single create method that accepts entity identifier and returns a new instance of the entity with a given identifier. In the example above, that implies that our Cart class has a constructor similar to this: <markup lang=\"java\" >public Cart(Long cartId) { this.cartId = cartId; } Just like with projections and other operations, in addition to update methods that can be used to modify a single entity, there are also a number of updateAll methods that can be used to modify multiple entities in a single call. An example where this may be useful is when you want to apply the same exact function to multiple entities, as is the case when performing stock split: <markup lang=\"java\" >positions.updateAll( Filters.equal(Position::getSymbol, \"AAPL\"), Position::split, 5); the Filter used to determine the set of positions to update the function to apply to each position; in this case split(5) will be called on each Position entity with AAPL symbol Just like with single-entity updates, the result of each function invocation will be returned to the client, this time in the form of a Map containing the identifiers of the processed entities as keys, and the result of the function applied to that entity as the value. ",
            "title": "In-place Updates"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " I mentioned earlier that Coherence can use indexes to optimize queries and aggregations. The indexes allow you to avoid deserializing entities stored across the cluster, which is a potentially expensive operation when you have large data set, with complex entity classes. The indexes themselves can also be sorted, which is helpful when executing range-based queries, such as less , greater or between . The standard way to create indexes is by calling NamedMap.addIndex method, which is certainly still an option. However, Repository API introduces a simpler, declarative way of index creation. To define an index, simply annotate the accessor for the entity attribute(s) that you&#8217;d like to create an index for with @Indexed annotation: <markup lang=\"java\" >public class Person { @Indexed public String getName() { return name; } @Indexed(ordered = true) public int getAge() { return age; } } defines an unordered index on Person::getName , which is suitable for filters such as equal , like , and regex defines an ordered index on Person::getAge , which is better suited for filters such as less , greater and between When the repository is created, it will introspect the entity class for @Indexed annotation and automatically create an index for each attribute that has one. The created index will then be used whenever that attribute is referenced within the query expression. In some cases you may want to keep deserialized entity instances around instead of discarding them. This can be useful when you are making frequent queries, aggregations, and using Stream API, or even in-place updates or projection, as the cost of maintaining individual indexes on all the attributes you need may end up being greater than to simply keep deserialized entity instances around. For situations like that Coherence provides a special index type you can use, DeserializationAccelerator , but if you are using Repository API you once again have an easier way of configuring it&#8201;&#8212;&#8201;simply annotate either the entity class, or the repository class itself with the @Accelerated annotation: <markup lang=\"java\" >@Accelerated public class Person { } Obviously, you will require additional storage capacity in your cluster in order to be able to store both the serialized and deserialized copy of all the entities, but in some situations the performance benefits can significantly outweigh the cost. In other words, acceleration is a classic example of a time vs. space tradeoff, and it is entirely up to you to decide when it makes sense to use it. ",
            "title": "Declarative Acceleration and Index Creation"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " We&#8217;ve already covered how you can query the repository to retrieve a subset of entities using a findAll method and a Filter , but sometimes you don&#8217;t need the entities themselves, but a result of some computation applied to a subset of entities in the repository. For example, you may need to calculate average salary of all the employees in a department, or the total value of all equity positions in a portfolio. While you could certainly query the repository for the entities that need to be processed and perform processing itself on the client, this is very inefficient way to accomplish the task, as you may end up moving significant amount of data over the network, just to discard it after the client-side processing. As you&#8217;ve probably noticed by now, Coherence provides a number of feature that allow you to perform various types of distributed processing efficiently, and this situation is no exception. Just like the in-place updates leverage Coherence Entry Processor API to perform data mutation on cluster members that store the data, Repository API support for data aggregation leverages Coherence Remote Stream API and the Aggregation API to perform read-only distributed computations efficiently. This once again allows you to move processing to the data, instead of the other way around, and to perform computation in parallel across as many CPU cores as your cluster has, instead of a handful of (or in many cases only one) cores on the client. The first option is to use the Stream API, which you are probably already familiar with because it&#8217;s a standard Java API introduced in Java 8. For example, you could calculate the average salary of all employees like this: <markup lang=\"java\" >double avgSalary = employees.stream() .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); If you wanted to calculate average salary only for the employees in a specific department instead, you could filter the employees to process: <markup lang=\"java\" >double avgSalary = employees.stream() .filter(e -&gt; e.getDepartmentId == departmentId) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); However, while it works, the code above is not ideal, as it will end up processing, and potentially deserializing all the employees in the repository in order to determine whether they belong to a specified department. A better way to accomplish the same task is to use Coherence-specific stream method overload which allows you to specify the Filter to create a stream based on: <markup lang=\"java\" >double avgSalary = employees.stream(Filters.equal(Employee::getDepartmentId, departmentId)) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); The difference is subtle, but important: unlike previous example, this allows Coherence to perform query before creating the stream, and leverage any indexes you may have in the process. This can significantly reduce the overhead when dealing with large data sets. However, there is also an easier way to accomplish the same thing: <markup lang=\"java\" >double avgSalary = employees.average(Employee::getSalary); or, for a specific department: <markup lang=\"java\" >double avgSalary = employees.average( Filters.equal(Employee::getDepartmentId, departmentId), Employee::getSalary); These are the examples of using repository aggregation methods directly, which turn common tasks such as finding min , max , average and sum of any entity attribute as simple as it can be. There are also more advanced aggregations, such as groupBy and top : <markup lang=\"java\" >Map&lt;Gender, Set&lt;Person&gt;&gt; peopleByGender = people.groupBy(Person::getGender); Map&lt;Long, Double&gt; avgSalaryByDept = employees.groupBy(Employee::getDepartmentId, averagingDouble(Employee::getSalary)); List&lt;Double&gt; top5salaries = employees.top(Employee::getSalary, 5); as well as the simpler ones, such as count and distinct . Finally, in many cases you may care not only about min , max or top values of a certain attribute, but also about which entities those values belong to. For those situations, you can use minBy , maxBy and topBy methods, which returns the entities containing minimum, maximum and top values of an attribute, respectively: <markup lang=\"java\" >Optional&lt;Person&gt; oldestPerson = people.maxBy(Person::getAge); Optional&lt;Person&gt; youngestPerson = people.minBy(Person::getAge); List&lt;Employee&gt; highestPaidEmployees = employees.topBy(Employee::getSalary, 5); Declarative Acceleration and Index Creation I mentioned earlier that Coherence can use indexes to optimize queries and aggregations. The indexes allow you to avoid deserializing entities stored across the cluster, which is a potentially expensive operation when you have large data set, with complex entity classes. The indexes themselves can also be sorted, which is helpful when executing range-based queries, such as less , greater or between . The standard way to create indexes is by calling NamedMap.addIndex method, which is certainly still an option. However, Repository API introduces a simpler, declarative way of index creation. To define an index, simply annotate the accessor for the entity attribute(s) that you&#8217;d like to create an index for with @Indexed annotation: <markup lang=\"java\" >public class Person { @Indexed public String getName() { return name; } @Indexed(ordered = true) public int getAge() { return age; } } defines an unordered index on Person::getName , which is suitable for filters such as equal , like , and regex defines an ordered index on Person::getAge , which is better suited for filters such as less , greater and between When the repository is created, it will introspect the entity class for @Indexed annotation and automatically create an index for each attribute that has one. The created index will then be used whenever that attribute is referenced within the query expression. In some cases you may want to keep deserialized entity instances around instead of discarding them. This can be useful when you are making frequent queries, aggregations, and using Stream API, or even in-place updates or projection, as the cost of maintaining individual indexes on all the attributes you need may end up being greater than to simply keep deserialized entity instances around. For situations like that Coherence provides a special index type you can use, DeserializationAccelerator , but if you are using Repository API you once again have an easier way of configuring it&#8201;&#8212;&#8201;simply annotate either the entity class, or the repository class itself with the @Accelerated annotation: <markup lang=\"java\" >@Accelerated public class Person { } Obviously, you will require additional storage capacity in your cluster in order to be able to store both the serialized and deserialized copy of all the entities, but in some situations the performance benefits can significantly outweigh the cost. In other words, acceleration is a classic example of a time vs. space tradeoff, and it is entirely up to you to decide when it makes sense to use it. ",
            "title": "Stream API and Data Aggregation"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Coherence not only allows you to store, modify, query and aggregate your data entities efficiently, but you can also register to receive event notifications whenever any entity in the repository changes. To do that, you can create and register a listener that will be notified whenever an entity is inserted, updated or removed: <markup lang=\"java\" > public static class PeopleListener implements PeopleRepository.Listener&lt;Person&gt; { public void onInserted(Person personNew) { // handle INSERT event } public void onUpdated(Person personOld, Person personNew) { // handle UPDATE event } public void onRemoved(Person personOld) { // handle REMOVE event } } <markup lang=\"java\" >people.addListener(new PeopleListener()); people.addListener(\"111-22-3333\", new PeopleListener()); people.addListener(Filters.greater(Person::getAge, 17), new PeopleListener()); registers a listener that will be notified whenever any entity in the repository is inserted, updated or removed registers a listener that will be notified when an entity with the specified identifier is inserted, updated or removed registers a listener that will be notified when any Person older than 17 is inserted, updated or removed As you can see from the example above, there are several ways to register only for the events you are interested in, in order to reduce the number of events received, and the amount of data sent over the network. Note that all of the listener methods above have a default no-op implementation, so you only need to implement the ones you actually want to handle. However, having to implement a separate class each time you want to register a listener is a bit cumbersome, so Repository API also provides a default listener implementation, and a fluent builder for it that make the task a bit easier: <markup lang=\"java\" >people.addListener( people.listener() .onInsert(personNew -&gt; { /* handle INSERT event */ }) .onUpdate((personOld, personNew) -&gt; { /* handle UPDATE event with old value */ }) .onUpdate(personNew -&gt; { /* handle UPDATE event without old value */ }) .onRemove(personOld -&gt; { /* handle REMOVE event */ }) .build() ); Note that when using Listener Builder API you have the option of omitting the old entity value from the onUpdate event handler arguments list. You can also specify multiple handlers for the same event type, in which case they will be composed and invoked in the specified order. There is also an option of providing a single event handler that will receive all the events, regardless of the event type: <markup lang=\"java\" >people.addListener( people.listener() .onEvent(person -&gt; { /* handle all events */ }) .build() ); Just like when implementing listener class explicitly, you can still pass entity identifier or a Filter as the first argument to addListener method in order to limit the scope of the events received. ",
            "title": "Event Listeners"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " The Coherence Repository API was introduced to make the implementation of data access layer within the applications easier, regardless of which framework you use to implement applications that use Coherence as a data store. It works equally well for plain Java applications and applications that use CDI, where you can simply create your own repository implementations, as described at the beginning of this document. It is also the foundation for our Micronaut Data and Spring Data repository implementations, so all the functionality described here is available when using those frameworks as well. The only difference is how you define your own repositories, which is framework-specific and documented separately. We hope you&#8217;ll find this new feature useful, and that it will make implementation of your Coherence-backed data access layers even easier. ",
            "title": "Summary"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Coherence Repository API provides a higher-level, DDD-friendly way to access data managed in Coherence. It is implemented on top of the existing NamedMap API, but it provides a number of features that make it easier to use for many typical use cases where Coherence is used as a Key-Value data store. Features and Benefits In addition to the basic CRUD functionality, the Repository API provides many features that simplify common data management tasks: Powerful projection features Flexible in-place entity updates First-class data aggregation support Stream API support Event listener support Declarative acceleration and index creation CDI Support Implementing a Repository Coherence provides an abstract base class com.oracle.coherence.repository.AbstractRepository , which your custom repository implementation needs to extend and provide implementation of the three abstract methods: <markup lang=\"java\" > /** * Return the {@link NamedMap} that is used as the underlying entity store. * * @return the {@link NamedMap} that is used as the underlying entity store */ protected abstract NamedMap&lt;ID, T&gt; getMap(); /** * Return the identifier of the specified entity instance. * * @param entity the entity to get the identifier from * * @return the identifier of the specified entity instance */ protected abstract ID getId(T entity); /** * Return the type of entities in this repository. * * @return the type of entities in this repository */ protected abstract Class&lt;? extends T&gt; getEntityType(); For example, a repository implementation that can be used to store Person entities, with String identifiers, can be as simple as: <markup lang=\"java\" >public class PeopleRepository extends AbstractRepository&lt;String, Person&gt; { private NamedMap&lt;String, Person&gt; people; public PeopleRepository(NamedMap&lt;String, Person&gt; people) { this.people = people; } protected NamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person person) { return person.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the NamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository That is it in a nutshell: a trivial repository implementation above will allow you to access all the Repository API features described in the remaining sections, which are provided by the AbstractRepository class you extended. However, you are free (and encouraged) to add additional business methods to the repository class above that will make it easier to use within your application. The most common example of such methods would be various \"finder\" methods that your application needs. For example, if your application needs to frequently query the repository to find people based on their name, you may want to add a method for that purpose: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return findAll(filter); } You can then invoke findByName method directly within the application to find all the people whose first or last name starts with a letter A , for example: <markup lang=\"java\" >for (Person p : people.findByName(\"A%\")) { // processing } Basic CRUD Operations We&#8217;ve already seen one read operation, findAll , in the example above, but let&#8217;s start from the beginning and look into how we can add, remove, update and query our repository. To add new entities to the repository, or replace the existing ones, you can use either the save or the saveAll method. The former takes a single entity as an argument and stores it in the backing NamedMap : <markup lang=\"java\" >people.save(new Person(\"555-11-2222\", \"Aleks\", 46)); The latter allows you to store a batch of entities at once by passing either an array, a collection or a stream of entities as an argument. Once you have some entities stored in a repository, you can query the repository using findById and findAll methods. <markup lang=\"java\" >Person person = people.findById(\"555-11-2222\"); assert person.getName().equals(\"Aleks\"); assert person.getAge() == 46; Collection&lt;Person&gt; allPeople = people.findAll(); Collection&lt;Person&gt; allAdults = people.findAll(Filters.greaterOrEqual(Person::getAge, 18)); find a single Person by identifier find all the people in the repository find all the people in the repository that are 18 or older When using findAll , you can also specify an optional sort order for the results, by specifying a Comparable property via a method reference: <markup lang=\"java\" >Collection&lt;Person&gt; peopleOrderedByAge = people.findAll(Person::getAge) the result will contain all people from the repository, sorted by age from the youngest to the oldest For more complex use cases, you can specify a Comparator to use instead. For example, if we wanted to always sort the results of the findByName method defined above first by last name and then by first name, we could re-implement it as: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return findAll(filter, Remote.comparator(Person::getLastName) .thenComparing(Person::getFirstName)); } the results will be sorted by last name, and then by first name; note that we are using Coherence Remote.comparator instead of standard Java Comparator in order to ensure that the specified comparator is serializable and can be sent to remote cluster members. Finally, to remove entities from a repository you can use one of several remove methods: <markup lang=\"java\" >boolean fRemoved = people.remove(person); boolean fRemoved = people.removeById(\"111-22-3333\"); removes specified entity from the repository removes entity with the specified identifier from the repository In both examples above the result will be a boolean indicating whether the entity was actually removed from the backing NamedMap , and it may be false if the entity wasn&#8217;t present in the repository. If you are interested in the removed value itself, you can use the overloads of the methods above that allow you to express that: <markup lang=\"java\" >Person removed = people.remove(person, true); Person removed = people.removeById(\"111-22-3333\", true); removes specified entity from the repository and returns it as the result removes entity with the specified identifier from the repository and returns it as the result Note that this will result in additional network traffic, so unless you really need the removed entity it is probably best not to ask for it. The examples above are useful when you want to remove a single entity from the repository. In cases when you want to remove multiple entities as part of a single network call, you should use one of removeAll methods instead, which allow you to remove a set of entities by specifying either their identifiers explicitly, or the criteria for removal via the Filter . <markup lang=\"java\" >boolean fChanged = people.removeAll(Filters.equal(Person::getGender, Gender.MALE)); boolean fChanged = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\")); removes all men from the repository and returns true if any entity has been removed removes entities with the specified identifiers from the repository and returns true if any entity has been removed Just like with single-entity removal operations, you can also use overloads that allow you to return the removed entities as the result: <markup lang=\"java\" >Map&lt;String, Person&gt; mapRemoved = people.removeAll(Filters.equal(Person::getGender, Gender.MALE), true); Map&lt;String, Person&gt; mapRemoved = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\"), true); removes all men from the repository and returns the map of removed entities, keyed by identifier removes entities with the specified identifiers from the repository and returns the map of removed entities, keyed by identifier Projection While querying repository for a collection of entities that satisfy some criteria is certainly a common and useful operation, sometimes you don&#8217;t need all the information contained within the entity. For example, if you only need a person&#8217;s name, querying for and then discarding all the information contained within the Person instances is unnecessary and wasteful. It is the equivalent of executing <markup lang=\"sql\" >SELECT * FROM PEOPLE against a relational database, when a simple <markup lang=\"sql\" >SELECT name FROM PEOPLE would suffice. Coherence Repository API allows you to limit the amount of data collected by performing server-side projection of the entity attributes you are interested in. For example, if you only need a person&#8217;s name, you can get just the name: <markup lang=\"java\" >String name = people.get(\"111-22-3333\", Person::getName); Map&lt;String, String&gt; mapNames = people.getAll(Filters.less(Person::getAge, 18), Person::getName); return the name of the person with a specified identifier return the map of names of all the people younger than 18, keyed by person&#8217;s identifier Obviously, returning either the whole entity or a single attribute from an entity are two ends of the spectrum, and more often than not you need something in between. For example, you may need the person&#8217;s name and age. For situations like that, Coherence allows you to use fragments : <markup lang=\"java\" >Fragment&lt;Person&gt; fragment = people.get(\"111-22-3333\", Person::getName, Person::getAge); String name = fragment.get(Person::getName); int age = fragment.get(Person::getAge); return a fragment containing the name and age of the person with a specified identifier retrieve the person&#8217;s name from a fragment retrieve the person&#8217;s age from a fragment You can, of course, perform the same projection across multiple entities using one of getAll methods: <markup lang=\"java\" >Map&lt;String, Fragment&lt;Person&gt;&gt; fragments = people.getAll( Filters.less(Person::getAge, 18), Person::getName, Person::getAge); return a map of fragments containing the name and age of all the people younger than 18, keyed by person&#8217;s identifier Unlike the relational database, which contains a set of columns for each row in the table, Coherence stores each entity as a full object graph, which means that the attributes can be other object graphs and can be nested to any level. This means that we also may need to be able to project attributes of the nested objects. For example, our Person class may have a nested Address object as an attribute, which in turn has street , city , and country attributes. If we want to retrieve the name and the country of a person in a repository, we can do it like this: <markup lang=\"java\" >Fragment&lt;Person&gt; person = people.get( \"111-22-3333\", Person::getName, Extractors.fragment(Person::getAddress, Address::getCountry)); String name = person.get(Person::getName); Fragment&lt;Address&gt; address = person.getFragment(Person::getAddress); String country = address.get(Address::getCountry); return a fragment containing the name and the Address fragment of the person with a specified identifier retrieve the person&#8217;s name from the Person fragment retrieve the Address fragment from the Person fragment retrieve the person&#8217;s country from the Address fragment In-place Updates By far the most common approach for updating data in modern applications is the read-modify-write pattern. For example, the typical code to update an attribute of a Person may look similar to the following: <markup lang=\"java\" >Person person = people.findById(\"111-22-3333\"); person.setAge(55); people.save(person); This is true regardless of whether the underlying data store provides a better, more efficient way of updating data. For example, RDBMS provide stored procedures for that purpose, but very few developers use them because they are not as convenient to use, and do not fit well into popular application frameworks, such as JPA, Spring Data or Micronaut Data. They also fragment the code base to some extent, splitting the business logic across the application and the data store, and require that some application code is written in SQL. However, the approach above is suboptimal, for a number of reasons: It at least doubles the number of network calls the application makes to the data store, increasing the overall latency of the operation. It moves (potentially a lot) more data over the network than absolutely necessary. It may require expensive construction of a complex entity in order to perform a very simple update operation of a single attribute (this is particularly true with JPA and RDBMS back ends). It puts additional, unnecessary load on the data store, which is typically the hardest component of the application to scale. It introduces concurrency issues (ie. what should happen if the entity in the data store changes between the initial read and subsequent write), which typically requires that both the read and the write happen within the same transaction. A much better, more efficient way to perform the updates is to send the update function to the data store, and execute it locally, within the data store itself (which is pretty much what stored procedures are for). Coherence has always had support for these types of updates via entry processors , but the Repository API makes it even simpler to do so. For example, the code above can be rewritten as: <markup lang=\"java\" >people.update(\"111-22-3333\", Person::setAge, 55); We are basically telling Coherence to update Person instance with a given identifier by calling setAge method on it with a number 55 as an argument. This is not only significantly more efficient, but I&#8217;m sure you&#8217;ll agree, shorter and easier to write, and to read. Note that we don&#8217;t know, or care, where in the cluster a Person instance with a given identifier is&#8201;&#8212;&#8201;all we care about is that Coherence guarantees that it will invoke the setAge method on the entity with a specified ID, on a primary owner , and automatically create a backup of the modified entity for fault tolerance. It is also worth pointing out that the approach above provides the same benefits stored procedures do in RDBMS, but without the downsides: you are still writing all your code in Java, and keeping it in the same place. As a matter of fact, this approach allows you to implement rich domain models for your data, and execute business logic on your entities remotely, which works exceptionally well with DDD applications. Calling a setter on an entity remotely is only the tip of the iceberg, and far from sufficient for all data mutation needs. For example, conventional JavaBean setter returns void , but you often want to know what the entity value is after the update. The solution to that problem is simple: Coherence will return the result of the specified method invocation, so all you need to do is change the setAge method to implement fluent API: <markup lang=\"java\" >public Person setAge(int age) { this.age = age; return this; } You will now get the modified Person instance as the result of the update call: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", Person::setAge, 55); assert person.getAge() == 55; Sometimes you need to perform more complex updates, or update multiple attributes at the same time. While you could certainly accomplish both of those by making multiple update calls, that is inefficient because each update will result in a separate network call. You are better off using the update overload that allows you to specify the function to execute in that situation: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", p -&gt; { p.setAge(55); p.setGender(Gender.MALE); return p; }); assert person.getAge() == 55; assert person.getGender() == Gender.MALE; This way you have full control of the update logic that will be executed, and the return value. You may sometimes want to update an entity that does not exist in the repository yet, in which case you want to create a new instance. For example, you may want to create a shopping cart entity for a customer when they add the first item to the cart. While you could implement the code to check whether the Cart for a given customer exists, and create new one if it doesn&#8217;t, this again results in network calls that can be avoided if you simply create the Cart instance as part of Cart::addItem call. The Repository API allows you to accomplish that via optional EntityFactory argument: <markup lang=\"java\" >carts.update(customerId, Cart::addItem, item, Cart::new); the cart/customer identifier the method to invoke on a target Cart instance the CartItem to add to the cart the EntityFactory to use to create a new Cart instance if the cart with the specified identifier doesn&#8217;t exist The EntityFactory interface is quite simple: <markup lang=\"java\" >@FunctionalInterface public interface EntityFactory&lt;ID, T&gt; extends Serializable { /** * Create an entity instance with the specified identity. * * @param id identifier to create entity instance with * * @return a created entity instance */ T create(ID id); } Basically, it has a single create method that accepts entity identifier and returns a new instance of the entity with a given identifier. In the example above, that implies that our Cart class has a constructor similar to this: <markup lang=\"java\" >public Cart(Long cartId) { this.cartId = cartId; } Just like with projections and other operations, in addition to update methods that can be used to modify a single entity, there are also a number of updateAll methods that can be used to modify multiple entities in a single call. An example where this may be useful is when you want to apply the same exact function to multiple entities, as is the case when performing stock split: <markup lang=\"java\" >positions.updateAll( Filters.equal(Position::getSymbol, \"AAPL\"), Position::split, 5); the Filter used to determine the set of positions to update the function to apply to each position; in this case split(5) will be called on each Position entity with AAPL symbol Just like with single-entity updates, the result of each function invocation will be returned to the client, this time in the form of a Map containing the identifiers of the processed entities as keys, and the result of the function applied to that entity as the value. Stream API and Data Aggregation We&#8217;ve already covered how you can query the repository to retrieve a subset of entities using a findAll method and a Filter , but sometimes you don&#8217;t need the entities themselves, but a result of some computation applied to a subset of entities in the repository. For example, you may need to calculate average salary of all the employees in a department, or the total value of all equity positions in a portfolio. While you could certainly query the repository for the entities that need to be processed and perform processing itself on the client, this is very inefficient way to accomplish the task, as you may end up moving significant amount of data over the network, just to discard it after the client-side processing. As you&#8217;ve probably noticed by now, Coherence provides a number of feature that allow you to perform various types of distributed processing efficiently, and this situation is no exception. Just like the in-place updates leverage Coherence Entry Processor API to perform data mutation on cluster members that store the data, Repository API support for data aggregation leverages Coherence Remote Stream API and the Aggregation API to perform read-only distributed computations efficiently. This once again allows you to move processing to the data, instead of the other way around, and to perform computation in parallel across as many CPU cores as your cluster has, instead of a handful of (or in many cases only one) cores on the client. The first option is to use the Stream API, which you are probably already familiar with because it&#8217;s a standard Java API introduced in Java 8. For example, you could calculate the average salary of all employees like this: <markup lang=\"java\" >double avgSalary = employees.stream() .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); If you wanted to calculate average salary only for the employees in a specific department instead, you could filter the employees to process: <markup lang=\"java\" >double avgSalary = employees.stream() .filter(e -&gt; e.getDepartmentId == departmentId) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); However, while it works, the code above is not ideal, as it will end up processing, and potentially deserializing all the employees in the repository in order to determine whether they belong to a specified department. A better way to accomplish the same task is to use Coherence-specific stream method overload which allows you to specify the Filter to create a stream based on: <markup lang=\"java\" >double avgSalary = employees.stream(Filters.equal(Employee::getDepartmentId, departmentId)) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); The difference is subtle, but important: unlike previous example, this allows Coherence to perform query before creating the stream, and leverage any indexes you may have in the process. This can significantly reduce the overhead when dealing with large data sets. However, there is also an easier way to accomplish the same thing: <markup lang=\"java\" >double avgSalary = employees.average(Employee::getSalary); or, for a specific department: <markup lang=\"java\" >double avgSalary = employees.average( Filters.equal(Employee::getDepartmentId, departmentId), Employee::getSalary); These are the examples of using repository aggregation methods directly, which turn common tasks such as finding min , max , average and sum of any entity attribute as simple as it can be. There are also more advanced aggregations, such as groupBy and top : <markup lang=\"java\" >Map&lt;Gender, Set&lt;Person&gt;&gt; peopleByGender = people.groupBy(Person::getGender); Map&lt;Long, Double&gt; avgSalaryByDept = employees.groupBy(Employee::getDepartmentId, averagingDouble(Employee::getSalary)); List&lt;Double&gt; top5salaries = employees.top(Employee::getSalary, 5); as well as the simpler ones, such as count and distinct . Finally, in many cases you may care not only about min , max or top values of a certain attribute, but also about which entities those values belong to. For those situations, you can use minBy , maxBy and topBy methods, which returns the entities containing minimum, maximum and top values of an attribute, respectively: <markup lang=\"java\" >Optional&lt;Person&gt; oldestPerson = people.maxBy(Person::getAge); Optional&lt;Person&gt; youngestPerson = people.minBy(Person::getAge); List&lt;Employee&gt; highestPaidEmployees = employees.topBy(Employee::getSalary, 5); Declarative Acceleration and Index Creation I mentioned earlier that Coherence can use indexes to optimize queries and aggregations. The indexes allow you to avoid deserializing entities stored across the cluster, which is a potentially expensive operation when you have large data set, with complex entity classes. The indexes themselves can also be sorted, which is helpful when executing range-based queries, such as less , greater or between . The standard way to create indexes is by calling NamedMap.addIndex method, which is certainly still an option. However, Repository API introduces a simpler, declarative way of index creation. To define an index, simply annotate the accessor for the entity attribute(s) that you&#8217;d like to create an index for with @Indexed annotation: <markup lang=\"java\" >public class Person { @Indexed public String getName() { return name; } @Indexed(ordered = true) public int getAge() { return age; } } defines an unordered index on Person::getName , which is suitable for filters such as equal , like , and regex defines an ordered index on Person::getAge , which is better suited for filters such as less , greater and between When the repository is created, it will introspect the entity class for @Indexed annotation and automatically create an index for each attribute that has one. The created index will then be used whenever that attribute is referenced within the query expression. In some cases you may want to keep deserialized entity instances around instead of discarding them. This can be useful when you are making frequent queries, aggregations, and using Stream API, or even in-place updates or projection, as the cost of maintaining individual indexes on all the attributes you need may end up being greater than to simply keep deserialized entity instances around. For situations like that Coherence provides a special index type you can use, DeserializationAccelerator , but if you are using Repository API you once again have an easier way of configuring it&#8201;&#8212;&#8201;simply annotate either the entity class, or the repository class itself with the @Accelerated annotation: <markup lang=\"java\" >@Accelerated public class Person { } Obviously, you will require additional storage capacity in your cluster in order to be able to store both the serialized and deserialized copy of all the entities, but in some situations the performance benefits can significantly outweigh the cost. In other words, acceleration is a classic example of a time vs. space tradeoff, and it is entirely up to you to decide when it makes sense to use it. Event Listeners Coherence not only allows you to store, modify, query and aggregate your data entities efficiently, but you can also register to receive event notifications whenever any entity in the repository changes. To do that, you can create and register a listener that will be notified whenever an entity is inserted, updated or removed: <markup lang=\"java\" > public static class PeopleListener implements PeopleRepository.Listener&lt;Person&gt; { public void onInserted(Person personNew) { // handle INSERT event } public void onUpdated(Person personOld, Person personNew) { // handle UPDATE event } public void onRemoved(Person personOld) { // handle REMOVE event } } <markup lang=\"java\" >people.addListener(new PeopleListener()); people.addListener(\"111-22-3333\", new PeopleListener()); people.addListener(Filters.greater(Person::getAge, 17), new PeopleListener()); registers a listener that will be notified whenever any entity in the repository is inserted, updated or removed registers a listener that will be notified when an entity with the specified identifier is inserted, updated or removed registers a listener that will be notified when any Person older than 17 is inserted, updated or removed As you can see from the example above, there are several ways to register only for the events you are interested in, in order to reduce the number of events received, and the amount of data sent over the network. Note that all of the listener methods above have a default no-op implementation, so you only need to implement the ones you actually want to handle. However, having to implement a separate class each time you want to register a listener is a bit cumbersome, so Repository API also provides a default listener implementation, and a fluent builder for it that make the task a bit easier: <markup lang=\"java\" >people.addListener( people.listener() .onInsert(personNew -&gt; { /* handle INSERT event */ }) .onUpdate((personOld, personNew) -&gt; { /* handle UPDATE event with old value */ }) .onUpdate(personNew -&gt; { /* handle UPDATE event without old value */ }) .onRemove(personOld -&gt; { /* handle REMOVE event */ }) .build() ); Note that when using Listener Builder API you have the option of omitting the old entity value from the onUpdate event handler arguments list. You can also specify multiple handlers for the same event type, in which case they will be composed and invoked in the specified order. There is also an option of providing a single event handler that will receive all the events, regardless of the event type: <markup lang=\"java\" >people.addListener( people.listener() .onEvent(person -&gt; { /* handle all events */ }) .build() ); Just like when implementing listener class explicitly, you can still pass entity identifier or a Filter as the first argument to addListener method in order to limit the scope of the events received. Summary The Coherence Repository API was introduced to make the implementation of data access layer within the applications easier, regardless of which framework you use to implement applications that use Coherence as a data store. It works equally well for plain Java applications and applications that use CDI, where you can simply create your own repository implementations, as described at the beginning of this document. It is also the foundation for our Micronaut Data and Spring Data repository implementations, so all the functionality described here is available when using those frameworks as well. The only difference is how you define your own repositories, which is framework-specific and documented separately. We hope you&#8217;ll find this new feature useful, and that it will make implementation of your Coherence-backed data access layers even easier. ",
            "title": "Repository API"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " Coherence MP Metrics provides support for [Eclipse MicroProfile Metrics] ( https://microprofile.io/project/eclipse/microprofile-metrics ) within Coherence cluster members. This is a very simple module that allows you to publish Coherence metrics into MicroProfile Metric Registries available at runtime, and adds Coherence-specific tags to all the metrics published within the process, in order to distinguish them on the monitoring server, such as Prometheus. ",
            "title": "Coherence MicroProfile Metrics"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " In order to use Coherence MP Metrics, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-metrics&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; That&#8217;s it&#8201;&#8212;&#8201;once the module above is in the class path, Coherence will discover MpMetricRegistryAdapter service it provides, and use it to publish all standard Coherence metrics to the vendor registry, and any user-defined application metrics to the application registry. All the metrics will be published as gauges, because they represent point-in-time values of various MBean attributes. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " There could be hundreds of members in a Coherence cluster, with each member publishing potentially the same set of metrics. There could also be many Coherence clusters in the environment, possibly publishing to the same monitoring server instance. In order to distinguish metrics coming from different clusters, as well as from different members of the same cluster, Coherence MP Metrics will automatically add several tags to ALL the metrics published within the process. The tags added are: Tag Name Tag Value cluster the cluster name site the site the member belongs to (if set) machine the machine member is on (if set) member the name of the member (if set) node_id the node ID of the member role the member&#8217;s role This ensures that the metrics published by one member do not collide with and overwrite the metrics published by another members, and allows you to query and aggregate metrics based on the values of the tags above if desired. ",
            "title": "Coherence Global Tags"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " Coherence gRPC proxy is the server-side implementation of the services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. ",
            "title": "Coherence gRPC Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The gRPC server will start automatically when com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ) is started. Typically, Coherence will be used as the application&#8217;s main class, alternatively an instance of Coherence can be started using the bootstrap API. When reviewing the log output, you should see the following two log messages: <markup lang=\"log\" >Coherence gRPC proxy is now listening for connections on 0.0.0.0:1408 Coherence gRPC in-process proxy 'default' is now listening for connections The service is now ready to process requests from one of the Coherence gRPC client implementations. ",
            "title": "Start the Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " In order to use Coherence gRPC Server, you need to declare it as a dependency of your project; for example if using Maven: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; or for Gradle: <markup lang=\"groovy\" >implementation 'com.oracle.coherence.ce:coherence-grpc-proxy:21.06-SNAPSHOT' Start the Server The gRPC server will start automatically when com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ) is started. Typically, Coherence will be used as the application&#8217;s main class, alternatively an instance of Coherence can be started using the bootstrap API. When reviewing the log output, you should see the following two log messages: <markup lang=\"log\" >Coherence gRPC proxy is now listening for connections on 0.0.0.0:1408 Coherence gRPC in-process proxy 'default' is now listening for connections The service is now ready to process requests from one of the Coherence gRPC client implementations. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The port the gRPC server listens on can be changed using the coherence.grpc.server.port system property, for example -Dcoherence.grpc.server.port=7001 will cause the server to bind to port 7001 . ",
            "title": "Set the Port"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The name used by the in-process server can be changed using the coherence.grpc.inprocess.name system property, for example -Dcoherence.grpc.inprocess.name=foo will set the in-process server name to foo . ",
            "title": "Set the In-Process Server Name"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " By default, the gRPC server runs in plaintext mode. The gRPC server can be configured to use TLS. The relevant key, cert and optional CA files must be provided, either on the classpath of file system. Assuming that the TLS key is in a file named /certs/server.key , and that the server TLS cert is in a file named /certs/server.pem then TLS can be configured with the following system properties. <markup >coherence.grpc.server.credentials=tls coherence.grpc.server.tls.key=/certs/server.key coherence.grpc.server.tls.cert=/certs/server.pem An optional server key file password can be provided if required, for example if the key file password is secret : <markup >coherence.grpc.server.tls.password=secret By default, the server does not require client certificates. The server can be configured to use mutual authentication to verify the client certificates. <markup >coherence.grpc.server.tls.client=REQUIRED The valid values for the coherence.grpc.server.tls.client system property are NONE , REQUIRED , or OPTIONAL . If a CA file is required to verify the client certs it can be provided to the server, for example if the CA file is called /certs/ca.pem : <markup >coherence.grpc.server.tls.ca=/certs/ca.pem ",
            "title": "Using TLS"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " It is possible to have full control over the configuration of the server by implementing the interface com.oracle.coherence.grpc.proxy.GrpcServerConfiguration . Implementations of this interface will be loaded using the Java ServiceLoader before the server starts allowing the ServerBuilder used to build both the server and in-process server to be modified. For example, the class below implements GrpcServerConfiguration and configures both servers to use transport security certificates. <markup lang=\"java\" title=\"MyServerConfig.java\" >package com.acme.application; import com.oracle.coherence.grpc.proxy.GrpcServerConfiguration; import io.grpc.ServerBuilder;import io.grpc.inprocess.InProcessServerBuilder; public class MyServerConfig implements GrpcServerConfiguration { public void configure(ServerBuilder&lt;?&gt; serverBuilder, InProcessServerBuilder inProcessServerBuilder) { File fileCert = new File(\"/grpc.crt\"); File fileKey = new File(\"grpc.key\"); serverBuilder.useTransportSecurity(fileCert, fileKey); inProcessServerBuilder.useTransportSecurity(fileCert, fileKey); } } For the Coherence gRPC proxy to find the above configuration class via the ServiceLoader a file named com.oracle.coherence.grpc.proxy.GrpcServerConfiguration needs to be added to application classes META-INF/services directory. <markup title=\"com.oracle.coherence.grpc.proxy.GrpcServerConfiguration\" >com.acme.application.MyServerConfig When the gRPC proxy starts it will now discover the MyServerConfig and will call it to modify the server builders. As well as security as in the example, other configuration such as interceptors and even additional gRPC services can be added to the server before it starts. ",
            "title": "Advanced gRPC Proxy Server Configuration"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The default gRPC server will listen for remote connections on port 1408 as well as in-process connections on an in-process server named default . Set the Port The port the gRPC server listens on can be changed using the coherence.grpc.server.port system property, for example -Dcoherence.grpc.server.port=7001 will cause the server to bind to port 7001 . Set the In-Process Server Name The name used by the in-process server can be changed using the coherence.grpc.inprocess.name system property, for example -Dcoherence.grpc.inprocess.name=foo will set the in-process server name to foo . Using TLS By default, the gRPC server runs in plaintext mode. The gRPC server can be configured to use TLS. The relevant key, cert and optional CA files must be provided, either on the classpath of file system. Assuming that the TLS key is in a file named /certs/server.key , and that the server TLS cert is in a file named /certs/server.pem then TLS can be configured with the following system properties. <markup >coherence.grpc.server.credentials=tls coherence.grpc.server.tls.key=/certs/server.key coherence.grpc.server.tls.cert=/certs/server.pem An optional server key file password can be provided if required, for example if the key file password is secret : <markup >coherence.grpc.server.tls.password=secret By default, the server does not require client certificates. The server can be configured to use mutual authentication to verify the client certificates. <markup >coherence.grpc.server.tls.client=REQUIRED The valid values for the coherence.grpc.server.tls.client system property are NONE , REQUIRED , or OPTIONAL . If a CA file is required to verify the client certs it can be provided to the server, for example if the CA file is called /certs/ca.pem : <markup >coherence.grpc.server.tls.ca=/certs/ca.pem Advanced gRPC Proxy Server Configuration It is possible to have full control over the configuration of the server by implementing the interface com.oracle.coherence.grpc.proxy.GrpcServerConfiguration . Implementations of this interface will be loaded using the Java ServiceLoader before the server starts allowing the ServerBuilder used to build both the server and in-process server to be modified. For example, the class below implements GrpcServerConfiguration and configures both servers to use transport security certificates. <markup lang=\"java\" title=\"MyServerConfig.java\" >package com.acme.application; import com.oracle.coherence.grpc.proxy.GrpcServerConfiguration; import io.grpc.ServerBuilder;import io.grpc.inprocess.InProcessServerBuilder; public class MyServerConfig implements GrpcServerConfiguration { public void configure(ServerBuilder&lt;?&gt; serverBuilder, InProcessServerBuilder inProcessServerBuilder) { File fileCert = new File(\"/grpc.crt\"); File fileKey = new File(\"grpc.key\"); serverBuilder.useTransportSecurity(fileCert, fileKey); inProcessServerBuilder.useTransportSecurity(fileCert, fileKey); } } For the Coherence gRPC proxy to find the above configuration class via the ServiceLoader a file named com.oracle.coherence.grpc.proxy.GrpcServerConfiguration needs to be added to application classes META-INF/services directory. <markup title=\"com.oracle.coherence.grpc.proxy.GrpcServerConfiguration\" >com.acme.application.MyServerConfig When the gRPC proxy starts it will now discover the MyServerConfig and will call it to modify the server builders. As well as security as in the example, other configuration such as interceptors and even additional gRPC services can be added to the server before it starts. ",
            "title": "Configuration"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " As already stated above, the Coherence gRPC server will be started automatically based on DefaultCacheServer lifecycle events. This behaviour can be disabled by setting the coherence.grpc.enabled system property to false , in which case a gRPC server will not be started. ",
            "title": "Disabling the gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If the coherence.grpc.enabled system property has been set to false , the gRPC server can be started manually by calling the start() method on the GrpcController singleton instance, for example: <markup lang=\"java\" >import com.oracle.coherence.grpc.proxy.GrpcServerController; public class MyApplication { public static void main(String[] args) { // do application initialisation... GrpcServerController.INSTANCE.start(); // do more application initialisation... } } The gRPC server can be stopped by calling the corresponding GrpcServerController.INSTANCE.stop() method. ",
            "title": "Programmatically starting the gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If you have application code that needs to run only after the gRPC server has started this can be achieved by using the GrpcServerController.whenStarted() method. This method returns a CompletionStage that will be completed when the gRPC server has started. <markup lang=\"java\" >GrpcServerController.INSTANCE.whenStarted().thenRun(() -&gt; { // run post-start code... System.out.println(\"The gRPC server has started\"); }); ",
            "title": "Waiting For gRPC Server Start"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If using the Helidon Microprofile server with the microprofile gRPC server enabled the Coherence gRPC proxy can be deployed into the Helidon gRPC server instead of the Coherence default gRPC server. For this behaviour to happen automatically just set the coherence.grpc.enabled system property to false , which will disable the built in server. A built-in GrpcMpExtension implementation will then deploy the proxy services to the Helidon gRPC server. When using the Helidon MP gRPC server, if the coherence.grpc.enabled system property has not been set to false , then both the Helidon gRPC server and the Coherence default gRPC server will start and could cause port binding issues unless they are both specifically configured to use different ports. ",
            "title": "Deploy the Proxy Service with Helidon Microprofile gRPC Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If you are running your own instance of a gRPC server and want to just deploy the Coherence gRPC proxy service to this server then that is possible. If manually deploying the service, ensure that auto-start of the Coherence gRPC server has been disabled by setting the system property coherence.grpc.enabled=false <markup lang=\"java\" >// Create your gRPC ServerBuilder ServerBuilder builder = ServerBuilder.forPort(port); // Obtain the Coherence gRPC services and add them to the builder List&lt;BindableService&gt; services = GrpcServerController.INSTANCE.createGrpcServices() services.forEach(serverBuilder::addService); // Build and start the server Server server = serverBuilder.build(); server.start(); ",
            "title": "Manually Deploy the gRPC Proxy Service"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " The coherence-micrometer module provides integration between Coherence metrics and Micrometer allowing Coherence metrics to be published via any of the Micrometer registries. ",
            "title": "Coherence Micrometer Metrics"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " In order to use Coherence Micrometer metrics, you need to declare the module as a dependency in your pom.xml and bind your Micrometer registry with the Coherence metrics adapter: <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-micrometer&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; The coherence-micrometer provides a Micrometer MeterBinder implementation class called CoherenceMicrometerMetrics . This class is a singleton and cannot be constructed, to access it use the CoherenceMicrometerMetrics.INSTANCE field. Micrometer provides many registry implementations to support different metrics applications and formats. For example, to bind Coherence metrics to the Micrometer PrometheusMeterRegistry , create the PrometheusMeterRegistry as documented in the Micrometer documentation , and call the CoherenceMicrometerMetrics class&#8217;s bindTo method: <markup lang=\"java\" >PrometheusMeterRegistry prometheusRegistry = new PrometheusMeterRegistry(PrometheusConfig.DEFAULT); // complete registy configuration... CoherenceMicrometerMetrics.INSTANCE.bindTo(prometheusRegistry); Micrometer registries can be bound to Coherence at any time, before or after Coherence starts. As Coherence creates or removed metrics they will be registered with or removed from the Micrometer registries. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " Micrometer has a global registry available which Coherence will bind to automatically if the coherence.micrometer.bind.to.global system property has been set to true (this property is false by default). ",
            "title": "Automatic Global Registry Binding"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " The com.tangol.net.Coherence contains a main method that allows it to be used to run a Coherence server as a more powerful to alternative DefaultCahceServer . <markup lang=\"bash\" >$ java -cp coherence.jar com.tangosol.net.Coherence Without any other configuration, the default Coherence instance started this way will run an identical server to that run using DefaultCahceServer . The steps above are covered in more detail below. ",
            "title": "Running A Coherence Server"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " When running Coherence if no configuration is specified the default behaviour is to use the default configuration file to configure Coherence. This behaviour still applies to the bootstrap API. If a Coherence instance is started without specifying any session configurations then a single default Session will be created. This default Session will wrap a ConfigurableCacheFactory that has been created from the default configuration file. The default file name is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig system property. When creating a CoherenceConfiguration the default session can be added using the SessionConfiguration.defaultSession() helper method. This method returns a SessionConfiguration configured to create the default Session . For example, in the code below the default session configuration is specifically added to the CoherenceConfiguration . <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); ",
            "title": "The Default Session"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " All sessions have a name that must be unique within the application. If a name has not been specified when the SessionConfiguration is built the default name of $Default$ will be used. A Coherence instance will fail to start if duplicate Session names exist. For example, this configuration will have the default name. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .build(); This configuration will have the name Test . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .name(\"Test\") .build(); ",
            "title": "Session Name"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " The most common type of session is a wrapper around a ConfigurableCacheFactory . When using the SessionConfiguration builder the configuration file URI is specified using the withConfigUri() method, that takes a string value specifiying the configuration file location. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"cache-config.xml\") .build(); The example above uses a configuration file a named cache-config.xml . If a configuration URI is not specified then the default value will be used. This value is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig System property. ",
            "title": "Session Configuration URI"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Coherence provides many types of events, examples of a few would be life-cycle events for Coherence itself, cache life-cycle events, cache entry events, partition events etc. These events can be listened to by implementing an EventInterceptor that receives specific types of event. Event interceptors can be registered with a Session as part of its configuration. For example, suppose there is an interceptor class in the application called CacheInterceptor that listens to CacheLifecycleEvent when caches get created or destroyed. This interceptor can be added to the session as shown below: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withInterceptor(new CacheInterceptor()) .build(); The interceptor will receive cache life-cycle events for all caches created using the session. ",
            "title": "Session Event Interceptors"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Scope is a concept that has been in Coherence for quite a while that allows services to be scoped and hence isolated from other services with the same name. For example multiple ConfigurableCacheFactory instances could be loaded from the same XML configuration file but given different scope names so that each CCF will have its own services in the cluster. Unless you require multiple Sessions, a scope will not generally be used in a configuration. A scope for a session can be configured using the configuration&#8217;s withScopeName() method, for example: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withScopeName(\"Test\") .build(); The session (and any ConfigurableCacheFactory it wraps) created from the configuration above will have a scope name of Test . It is possible to set a scope name in the &lt;defaults&gt; section of the XML configuration file. <markup lang=\"xml\" title=\"scoped-configuration.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;defaults&gt; &lt;scope-name&gt;Test&lt;/scope-name&gt; &lt;/defaults&gt; A ConfigurableCacheFactory created from the XML above, and hence any Session that wraps it will have a scope of Test . Note When using the bootstrap API any scope name specifically configured in the SessionConfiguration (that is not the default scope name) will override the scope name in the XML file. For example, using the scoped-configuration.xml file above: In this case the scope name will be Foo because the scope name has been explicitly set in the SessionConfiguration . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .withScopeName(\"Foo\") .build(); In this case the scope name will be Foo because although no scope name has been explicitly set in the SessionConfiguration , the name has been set to Foo , so the scope name will default to Foo . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as no scope name or session name has been explicitly set in the SessionConfiguration so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as the session name has been set to Foo but the scope name has been explicitly set to the default scope name using the constant Coherence.DEFAULT_SCOPE so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(Coherence.DEFAULT_SCOPE) .withConfigUri(\"scoped-configuration.xml\") .build(); ",
            "title": "Session Scope"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A SessionConfiguration is created by using the SessionConfiguration builder as shown in the example above. The Default Session When running Coherence if no configuration is specified the default behaviour is to use the default configuration file to configure Coherence. This behaviour still applies to the bootstrap API. If a Coherence instance is started without specifying any session configurations then a single default Session will be created. This default Session will wrap a ConfigurableCacheFactory that has been created from the default configuration file. The default file name is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig system property. When creating a CoherenceConfiguration the default session can be added using the SessionConfiguration.defaultSession() helper method. This method returns a SessionConfiguration configured to create the default Session . For example, in the code below the default session configuration is specifically added to the CoherenceConfiguration . <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); Session Name All sessions have a name that must be unique within the application. If a name has not been specified when the SessionConfiguration is built the default name of $Default$ will be used. A Coherence instance will fail to start if duplicate Session names exist. For example, this configuration will have the default name. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .build(); This configuration will have the name Test . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .name(\"Test\") .build(); Session Configuration URI The most common type of session is a wrapper around a ConfigurableCacheFactory . When using the SessionConfiguration builder the configuration file URI is specified using the withConfigUri() method, that takes a string value specifiying the configuration file location. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"cache-config.xml\") .build(); The example above uses a configuration file a named cache-config.xml . If a configuration URI is not specified then the default value will be used. This value is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig System property. Session Event Interceptors Coherence provides many types of events, examples of a few would be life-cycle events for Coherence itself, cache life-cycle events, cache entry events, partition events etc. These events can be listened to by implementing an EventInterceptor that receives specific types of event. Event interceptors can be registered with a Session as part of its configuration. For example, suppose there is an interceptor class in the application called CacheInterceptor that listens to CacheLifecycleEvent when caches get created or destroyed. This interceptor can be added to the session as shown below: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withInterceptor(new CacheInterceptor()) .build(); The interceptor will receive cache life-cycle events for all caches created using the session. Session Scope Scope is a concept that has been in Coherence for quite a while that allows services to be scoped and hence isolated from other services with the same name. For example multiple ConfigurableCacheFactory instances could be loaded from the same XML configuration file but given different scope names so that each CCF will have its own services in the cluster. Unless you require multiple Sessions, a scope will not generally be used in a configuration. A scope for a session can be configured using the configuration&#8217;s withScopeName() method, for example: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withScopeName(\"Test\") .build(); The session (and any ConfigurableCacheFactory it wraps) created from the configuration above will have a scope name of Test . It is possible to set a scope name in the &lt;defaults&gt; section of the XML configuration file. <markup lang=\"xml\" title=\"scoped-configuration.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;defaults&gt; &lt;scope-name&gt;Test&lt;/scope-name&gt; &lt;/defaults&gt; A ConfigurableCacheFactory created from the XML above, and hence any Session that wraps it will have a scope of Test . Note When using the bootstrap API any scope name specifically configured in the SessionConfiguration (that is not the default scope name) will override the scope name in the XML file. For example, using the scoped-configuration.xml file above: In this case the scope name will be Foo because the scope name has been explicitly set in the SessionConfiguration . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .withScopeName(\"Foo\") .build(); In this case the scope name will be Foo because although no scope name has been explicitly set in the SessionConfiguration , the name has been set to Foo , so the scope name will default to Foo . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as no scope name or session name has been explicitly set in the SessionConfiguration so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as the session name has been set to Foo but the scope name has been explicitly set to the default scope name using the constant Coherence.DEFAULT_SCOPE so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(Coherence.DEFAULT_SCOPE) .withConfigUri(\"scoped-configuration.xml\") .build(); ",
            "title": "Session Configurations"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A Coherence instance manages one or more Session instances, which are added to the CoherenceConfiguration by adding the SessionConfiguration instances to the builder. If no sessions have been added to the builder the Coherence instance will run a single Session that uses the default configuration file. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); The configuration above will configure a Coherence instance with the default name and with a single Sessions that wil use the default configuration file. The default session can also be explicitly added to the CoherenceConfiguration : <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); As already shown, other session configurations may also be added to the CoherenceConfiguration : <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(session) .build(); Whilst there is no limit to the number of sessions that can be configured the majority of applications would only ever require a single session - more than likely just the default session. ",
            "title": "Adding Sessions"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A CoherenceConfiguration can be configured to automatically discover SessionConfiguration instances. These are discovered using the Java ServiceLoader . Any instances of SessionConfiguration or SessionConfiguration.Provider configured as services in META-INF/services/ files will be loaded. This is useful if you are building modular applications where you want to include functionality in a separate application module that uses its own Session . The SessionConfiguration for the module is made discoverable by the ServiceLoader then whenever the module&#8217;s jar file is on the classpath the Session will be created, and the module&#8217;s functionality will be available to the application. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .discoverSessions() .build(); The call to discoverSessions() will load discovered SessionConfiguration instances. ",
            "title": "Session Configuration Auto-Discovery"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Each Coherence instance must be uniquely named. A name can be specified using the named() method on the builder, if no name has been specified the default name of $Default$ will be used. In the majority of use-cases an application would only ever require a single Coherence instance so there would be no requirement to specify a name. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); The configuration above will create a Coherence instance with the name Carts . ",
            "title": "Coherence Instance Name"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " As already mentioned, event interceptors can be added to a SessionConfiguration to receive events for a session. Event interceptors can also be added to the Coherence instance to receive events for all Session instances managed by that Coherence instance. For example, reusing the previous CacheInterceptor class, but this time for caches in all sessions: <markup lang=\"java\" >SessionConfiguration cartsSession = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(cartsSession) .withInterceptor(new CacheInterceptor()) .build(); Now the CacheInterceptor will receive events for both the default session and the Certs session. ",
            "title": "Add Global Event Interceptors"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A Coherence application is started by creating a Coherence instance from a CoherenceConfiguration . An instance of CoherenceConfiguration is created using the builder. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Adding Sessions A Coherence instance manages one or more Session instances, which are added to the CoherenceConfiguration by adding the SessionConfiguration instances to the builder. If no sessions have been added to the builder the Coherence instance will run a single Session that uses the default configuration file. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); The configuration above will configure a Coherence instance with the default name and with a single Sessions that wil use the default configuration file. The default session can also be explicitly added to the CoherenceConfiguration : <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); As already shown, other session configurations may also be added to the CoherenceConfiguration : <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(session) .build(); Whilst there is no limit to the number of sessions that can be configured the majority of applications would only ever require a single session - more than likely just the default session. Session Configuration Auto-Discovery A CoherenceConfiguration can be configured to automatically discover SessionConfiguration instances. These are discovered using the Java ServiceLoader . Any instances of SessionConfiguration or SessionConfiguration.Provider configured as services in META-INF/services/ files will be loaded. This is useful if you are building modular applications where you want to include functionality in a separate application module that uses its own Session . The SessionConfiguration for the module is made discoverable by the ServiceLoader then whenever the module&#8217;s jar file is on the classpath the Session will be created, and the module&#8217;s functionality will be available to the application. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .discoverSessions() .build(); The call to discoverSessions() will load discovered SessionConfiguration instances. Coherence Instance Name Each Coherence instance must be uniquely named. A name can be specified using the named() method on the builder, if no name has been specified the default name of $Default$ will be used. In the majority of use-cases an application would only ever require a single Coherence instance so there would be no requirement to specify a name. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); The configuration above will create a Coherence instance with the name Carts . Add Global Event Interceptors As already mentioned, event interceptors can be added to a SessionConfiguration to receive events for a session. Event interceptors can also be added to the Coherence instance to receive events for all Session instances managed by that Coherence instance. For example, reusing the previous CacheInterceptor class, but this time for caches in all sessions: <markup lang=\"java\" >SessionConfiguration cartsSession = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(cartsSession) .withInterceptor(new CacheInterceptor()) .build(); Now the CacheInterceptor will receive events for both the default session and the Certs session. ",
            "title": "Coherence Configuration"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " It is possible to create a Coherence instance without specifying any configuration. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(); <markup lang=\"java\" >Coherence coherence = Coherence.client(); In both of the above examples the Coherence instance will have the default Session and any discovered sessions . ",
            "title": "Create a Default Coherence Instance"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A CoherenceConfiguration can be used to create a Coherence instance. A Coherence instance is created in one of two modes, either cluster member or client. The mode chosen affects how some types of Session are created and whether auto-start services are started. As the name suggests a \"cluster member\" is a Coherence instance that expects to start or join a Coherence cluster. In a cluster member any Session that wraps a ConfigurableCacheFactory will be have its services auto-started and monitored (this is the same behaviour that would have happened when using DefaultCacheServer to start a server). A \"client\" Coherence instance is typically not a cluster member, i.e. it is a Coherence*Extend or gRPC client. As such, Session instances that wrap a ConfigurableCacheFactory will not be auto-started, they will start on demand as resources such as maps, caches or topics are requested from them. The com.tangosol.net.Coherence class has static factory methods to create Coherence instances in different modes. For example, to create a Coherence instance that is a cluster member the Coherence.clusterMember method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.clusterMember(cfg); For example, to create a Coherence instance that is a client the Coherence.client method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.client(cfg); Create a Default Coherence Instance It is possible to create a Coherence instance without specifying any configuration. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(); <markup lang=\"java\" >Coherence coherence = Coherence.client(); In both of the above examples the Coherence instance will have the default Session and any discovered sessions . ",
            "title": "Create a Coherence Instance"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A Coherence instance it must be started to start all the sessions that the Coherence instance is managing. This is done by calling the start() method. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(cfg); coherence.start(); ",
            "title": "Start Coherence"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " To avoid having to pass around the instance of Coherence that was used to bootstrap an application the Coherence class has some static methods that make it simple to retrieve an instance. If only a single instance of Coherence is being used in an application (which will cover most use-cases) then the getInstance() method can be used: <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); It is also possible to retrieve an instance by name: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); Coherence.create(cfg); &#8230;&#8203;then later&#8230;&#8203; <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); ",
            "title": "Obtaining a Coherence Instance"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " If application code needs to ensure that a Coherence instance has started before doing some work then the whenStarted() method can be used to obtain a CompletableFuture that will be completed when the Coherence instance has started. <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); CompletableFuture&lt;Void&gt; future = coherence.whenStarted(); future.join(); There is also a corresponding whenStopped() method that returns a future that will be completed when the Coherence instance stops. ",
            "title": "Ensuring Coherence Has Started"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Besides using the future methods described above it is possible to add and EventInterceptor to the configuration of a Coherence instance that will receive life-cycle events. Below is an example interceptor that implements Coherence.LifecycleListener . <markup lang=\"java\" >public class MyInterceptor implements Coherence.LifecycleListener { public void onEvent(CoherenceLifecycleEvent event) { // process event } } The interceptor can be added to the configuration: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withInterceptor(new MyInterceptor()) .build(); When a Coherence instance created from this configuration is start or stopped the MyInterceptor instance will receive events. ",
            "title": "Coherence Lifecycle Interceptors"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Coherence has a simple bootstrap API that allows a Coherence application to be configured and started by building a com.tangol.net.Coherence instance and starting it. The Coherence instance provides access to one or more com.tangosol.net.Session instances. A com.tangosol.net.Session gives access to Coherence clustered resources, such as NamedMap , NamedCache , NamedTopic etc. Sessions can be of different types, for example a session can be related to a ConfigurableCacheFactory , itself configured from a configuration file, or a session might be a client-side gRPC session. An example of some application bootstrap code might look like this: <markup lang=\"java\" >import com.tangosol.net.Coherence; import com.tangosol.net.CoherenceConfiguration; import com.tangosol.net.SessionConfiguration; public class Main { public static void main(String[] args) { // Create a Session configuration SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); // Create a Coherence instance configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(session) .build(); // Create the Coherence instance from the configuration Coherence coherence = Coherence.clusterMember(cfg); // Start Coherence coherence.start(); } } A SessionConfiguration is created. In this case the Session will be named Carts and will be created from the cache-config.xml configuration file. A CoherenceConfiguration is created to configure the Coherence instance. This configuration contains the Carts session configuration. A Coherence cluster member instance is created from the CoherenceConfiguration The Coherence instance is started. Running A Coherence Server The com.tangol.net.Coherence contains a main method that allows it to be used to run a Coherence server as a more powerful to alternative DefaultCahceServer . <markup lang=\"bash\" >$ java -cp coherence.jar com.tangosol.net.Coherence Without any other configuration, the default Coherence instance started this way will run an identical server to that run using DefaultCahceServer . The steps above are covered in more detail below. Session Configurations A SessionConfiguration is created by using the SessionConfiguration builder as shown in the example above. The Default Session When running Coherence if no configuration is specified the default behaviour is to use the default configuration file to configure Coherence. This behaviour still applies to the bootstrap API. If a Coherence instance is started without specifying any session configurations then a single default Session will be created. This default Session will wrap a ConfigurableCacheFactory that has been created from the default configuration file. The default file name is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig system property. When creating a CoherenceConfiguration the default session can be added using the SessionConfiguration.defaultSession() helper method. This method returns a SessionConfiguration configured to create the default Session . For example, in the code below the default session configuration is specifically added to the CoherenceConfiguration . <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); Session Name All sessions have a name that must be unique within the application. If a name has not been specified when the SessionConfiguration is built the default name of $Default$ will be used. A Coherence instance will fail to start if duplicate Session names exist. For example, this configuration will have the default name. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .build(); This configuration will have the name Test . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .name(\"Test\") .build(); Session Configuration URI The most common type of session is a wrapper around a ConfigurableCacheFactory . When using the SessionConfiguration builder the configuration file URI is specified using the withConfigUri() method, that takes a string value specifiying the configuration file location. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"cache-config.xml\") .build(); The example above uses a configuration file a named cache-config.xml . If a configuration URI is not specified then the default value will be used. This value is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig System property. Session Event Interceptors Coherence provides many types of events, examples of a few would be life-cycle events for Coherence itself, cache life-cycle events, cache entry events, partition events etc. These events can be listened to by implementing an EventInterceptor that receives specific types of event. Event interceptors can be registered with a Session as part of its configuration. For example, suppose there is an interceptor class in the application called CacheInterceptor that listens to CacheLifecycleEvent when caches get created or destroyed. This interceptor can be added to the session as shown below: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withInterceptor(new CacheInterceptor()) .build(); The interceptor will receive cache life-cycle events for all caches created using the session. Session Scope Scope is a concept that has been in Coherence for quite a while that allows services to be scoped and hence isolated from other services with the same name. For example multiple ConfigurableCacheFactory instances could be loaded from the same XML configuration file but given different scope names so that each CCF will have its own services in the cluster. Unless you require multiple Sessions, a scope will not generally be used in a configuration. A scope for a session can be configured using the configuration&#8217;s withScopeName() method, for example: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withScopeName(\"Test\") .build(); The session (and any ConfigurableCacheFactory it wraps) created from the configuration above will have a scope name of Test . It is possible to set a scope name in the &lt;defaults&gt; section of the XML configuration file. <markup lang=\"xml\" title=\"scoped-configuration.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;defaults&gt; &lt;scope-name&gt;Test&lt;/scope-name&gt; &lt;/defaults&gt; A ConfigurableCacheFactory created from the XML above, and hence any Session that wraps it will have a scope of Test . Note When using the bootstrap API any scope name specifically configured in the SessionConfiguration (that is not the default scope name) will override the scope name in the XML file. For example, using the scoped-configuration.xml file above: In this case the scope name will be Foo because the scope name has been explicitly set in the SessionConfiguration . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .withScopeName(\"Foo\") .build(); In this case the scope name will be Foo because although no scope name has been explicitly set in the SessionConfiguration , the name has been set to Foo , so the scope name will default to Foo . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as no scope name or session name has been explicitly set in the SessionConfiguration so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as the session name has been set to Foo but the scope name has been explicitly set to the default scope name using the constant Coherence.DEFAULT_SCOPE so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(Coherence.DEFAULT_SCOPE) .withConfigUri(\"scoped-configuration.xml\") .build(); Coherence Configuration A Coherence application is started by creating a Coherence instance from a CoherenceConfiguration . An instance of CoherenceConfiguration is created using the builder. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Adding Sessions A Coherence instance manages one or more Session instances, which are added to the CoherenceConfiguration by adding the SessionConfiguration instances to the builder. If no sessions have been added to the builder the Coherence instance will run a single Session that uses the default configuration file. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); The configuration above will configure a Coherence instance with the default name and with a single Sessions that wil use the default configuration file. The default session can also be explicitly added to the CoherenceConfiguration : <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); As already shown, other session configurations may also be added to the CoherenceConfiguration : <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(session) .build(); Whilst there is no limit to the number of sessions that can be configured the majority of applications would only ever require a single session - more than likely just the default session. Session Configuration Auto-Discovery A CoherenceConfiguration can be configured to automatically discover SessionConfiguration instances. These are discovered using the Java ServiceLoader . Any instances of SessionConfiguration or SessionConfiguration.Provider configured as services in META-INF/services/ files will be loaded. This is useful if you are building modular applications where you want to include functionality in a separate application module that uses its own Session . The SessionConfiguration for the module is made discoverable by the ServiceLoader then whenever the module&#8217;s jar file is on the classpath the Session will be created, and the module&#8217;s functionality will be available to the application. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .discoverSessions() .build(); The call to discoverSessions() will load discovered SessionConfiguration instances. Coherence Instance Name Each Coherence instance must be uniquely named. A name can be specified using the named() method on the builder, if no name has been specified the default name of $Default$ will be used. In the majority of use-cases an application would only ever require a single Coherence instance so there would be no requirement to specify a name. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); The configuration above will create a Coherence instance with the name Carts . Add Global Event Interceptors As already mentioned, event interceptors can be added to a SessionConfiguration to receive events for a session. Event interceptors can also be added to the Coherence instance to receive events for all Session instances managed by that Coherence instance. For example, reusing the previous CacheInterceptor class, but this time for caches in all sessions: <markup lang=\"java\" >SessionConfiguration cartsSession = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(cartsSession) .withInterceptor(new CacheInterceptor()) .build(); Now the CacheInterceptor will receive events for both the default session and the Certs session. Create a Coherence Instance A CoherenceConfiguration can be used to create a Coherence instance. A Coherence instance is created in one of two modes, either cluster member or client. The mode chosen affects how some types of Session are created and whether auto-start services are started. As the name suggests a \"cluster member\" is a Coherence instance that expects to start or join a Coherence cluster. In a cluster member any Session that wraps a ConfigurableCacheFactory will be have its services auto-started and monitored (this is the same behaviour that would have happened when using DefaultCacheServer to start a server). A \"client\" Coherence instance is typically not a cluster member, i.e. it is a Coherence*Extend or gRPC client. As such, Session instances that wrap a ConfigurableCacheFactory will not be auto-started, they will start on demand as resources such as maps, caches or topics are requested from them. The com.tangosol.net.Coherence class has static factory methods to create Coherence instances in different modes. For example, to create a Coherence instance that is a cluster member the Coherence.clusterMember method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.clusterMember(cfg); For example, to create a Coherence instance that is a client the Coherence.client method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.client(cfg); Create a Default Coherence Instance It is possible to create a Coherence instance without specifying any configuration. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(); <markup lang=\"java\" >Coherence coherence = Coherence.client(); In both of the above examples the Coherence instance will have the default Session and any discovered sessions . Start Coherence A Coherence instance it must be started to start all the sessions that the Coherence instance is managing. This is done by calling the start() method. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(cfg); coherence.start(); Obtaining a Coherence Instance To avoid having to pass around the instance of Coherence that was used to bootstrap an application the Coherence class has some static methods that make it simple to retrieve an instance. If only a single instance of Coherence is being used in an application (which will cover most use-cases) then the getInstance() method can be used: <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); It is also possible to retrieve an instance by name: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); Coherence.create(cfg); &#8230;&#8203;then later&#8230;&#8203; <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); Ensuring Coherence Has Started If application code needs to ensure that a Coherence instance has started before doing some work then the whenStarted() method can be used to obtain a CompletableFuture that will be completed when the Coherence instance has started. <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); CompletableFuture&lt;Void&gt; future = coherence.whenStarted(); future.join(); There is also a corresponding whenStopped() method that returns a future that will be completed when the Coherence instance stops. Coherence Lifecycle Interceptors Besides using the future methods described above it is possible to add and EventInterceptor to the configuration of a Coherence instance that will receive life-cycle events. Below is an example interceptor that implements Coherence.LifecycleListener . <markup lang=\"java\" >public class MyInterceptor implements Coherence.LifecycleListener { public void onEvent(CoherenceLifecycleEvent event) { // process event } } The interceptor can be added to the configuration: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withInterceptor(new MyInterceptor()) .build(); When a Coherence instance created from this configuration is start or stopped the MyInterceptor instance will receive events. ",
            "title": "Bootstrap API"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config provides support for Eclipse MicroProfile Config within Coherence cluster members. It allows you both to configure various Coherence parameters from the values specified in any of the supported config sources, and to use Coherence cache as another, mutable config source. ",
            "title": "Coherence MicroProfile Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " In order to use Coherence MP Config, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-config&lt;/artifactId&gt; &lt;version&gt;21.06-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; You will also need an implementation of the Eclipse MP Config specification as a dependency. For example, if you are using Helidon , add the following to your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-config&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- optional: add it if you want YAML config file support --&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-config-yaml&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence provides a number of configuration properties that can be specified by the users in order to define certain attributes or to customize cluster member behavior at runtime. For example, attributes such as cluster and role name, as well as whether a cluster member should or should not store data, can be specified via system properties: -Dcoherence.cluster=MyCluster -Dcoherence.role=Proxy -Dcoherence.distributed.localstorage=false Most of these attributes can also be defined within the operational or cache configuration file. For example, you could define first two attributes, cluster name and role, within the operational config override file: <markup lang=\"xml\" > &lt;cluster-config&gt; &lt;member-identity&gt; &lt;cluster-name&gt;MyCluster&lt;/cluster-name&gt; &lt;role-name&gt;Proxy&lt;/role-name&gt; &lt;/member-identity&gt; &lt;/cluster-config&gt; While these two options are more than enough in most cases, there are some issues with them being the only way to configure Coherence: When you are using one of Eclipse MicroProfile implementations, such as Helidon as the foundation of your application, it would be nice to define some of Coherence configuration parameters along with your other configuration parameters, and not in the separate file or via system properties. In some environments, such as Kubernetes, Java system properties are cumbersome to use, and environment variables are a preferred way of passing configuration properties to containers. Unfortunately, neither of the two use cases above is supported out of the box, but that&#8217;s the gap Coherence MP Config is designed to fill. As long as you have coherence-mp-config and an implementation of Eclipse MP Config specification to your class path, Coherence will use any of the standard or custom config sources to resolve various configuration options it understands. Standard config sources in MP Config include META-INF/microprofile-config.properties file, if present in the class path, environment variables, and system properties (in that order, with the properties in the latter overriding the ones from the former). That will directly address problem #2 above, and allow you to specify Coherence configuration options via environment variables within Kubernetes YAML files, for example: <markup lang=\"yaml\" > containers: - name: my-app image: my-company/my-app:1.0.0 env: - name: COHERENCE_CLUSTER value: \"MyCluster\" - name: COHERENCE_ROLE value: \"Proxy\" - name: COHERENCE_DISTRIBUTED_LOCALSTORAGE value: \"false\" Of course, the above is just an example&#8201;&#8212;&#8201;if you are running your Coherence cluster in Kubernetes, you should really be using Coherence Operator instead, as it will make both the configuration and the operation of your Coherence cluster much easier. You will also be able to specify Coherence configuration properties along with the other configuration properties of your application, which will allow you to keep everything in one place, and not scattered across many files. For example, if you are writing a Helidon application, you can simply add coherence section to your application.yaml : <markup lang=\"yaml\" >coherence: cluster: MyCluster role: Proxy distributed: localstorage: false ",
            "title": "Configuring Coherence using MP Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config also provides an implementation of Eclipse MP Config ConfigSource interface, which allows you to store configuration parameters in a Coherence cache. This has several benefits: Unlike pretty much all of the default configuration sources, which are static, configuration options stored in a Coherence cache can be modified without forcing you to rebuild your application JARs or Docker images. You can change the value in one place, and it will automatically be visible and up to date on all the members. While the features above give you incredible amount of flexibility, we also understand that such flexibility is not always desired, and the feature is disabled by default. If you want to enable it, you need to do so explicitly, by registering CoherenceConfigSource as a global interceptor in your cache configuration file: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.mp.config.CoherenceConfigSource&lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;!-- your cache mappings and schemes... --&gt; &lt;/cache-config&gt; Once you do that, CoherenceConfigSource will be activated as soon as your cache factory is initialized, and injected into the list of available config sources for your application to use via standard MP Config APIs. By default, it will be configured with a priority (ordinal) of 500, making it higher priority than all the standard config sources, thus allowing you to override the values provided via config files, environment variables and system properties. However, you have full control over that behavior and can specify different ordinal via coherence.mp.config.source.ordinal configuration property. ",
            "title": "Using Coherence Cache as a Config Source"
        }
 ]
}