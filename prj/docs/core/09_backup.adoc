///////////////////////////////////////////////////////////////////////////////
    Copyright (c) 2000, 2021, Oracle and/or its affiliates.

    Licensed under the Universal Permissive License v 1.0 as shown at
    http://oss.oracle.com/licenses/upl.
///////////////////////////////////////////////////////////////////////////////
= Partition Backup Enhancements
:description: Coherence Backup Enhancements
:keywords: coherence, distributed, java, documentation

// DO NOT remove this header - it might look like a duplicate of the header above, but
// they both serve a purpose, and the docs will look wrong if it is removed.
== Partition Backup Enhancements

Coherence partitioned caches maintain primary and backup partitions. The backup partitions
allow the application to simultaneously lose `backup-count` number of members. These backups
are kept as 'strong' as possible by ensuring they are on different machines, racks or sites.
In addition customers can choose to forgo backup synchronicity for write performance by enabling
asynchronous backups.

Described below are two features introduced in `21.12` that either capitalize on these backups to 
improve read throughput and/or latency, or optimize the asynchronous processing to increase write
throughput. These feature are called 'Read Locator' and 'Scheduled Backups' respectively.

== Read Locator

Prior to this change all Coherence {javadoc-root}/com/tangosol/net/NamedMap.html[NamedMap] requests are serviced by the primary owner of
the associated partition(s) (ignoring client side caches, i.e. {javadoc-root}/com/tangosol/net/cache/NearCache.html[NearCache] / {javadoc-root}/com/tangosol/net/cache/ContinuousQueryCache.html[CQC]). The `read-locator`
feature allows for certain requests (currently only {javadoc-root}/com/tangosol/util/ConcurrentMap.html#get(java.lang.Object)[NamedMap.get] or {javadoc-root}/com/tangosol/net/NamedMap.html#getAll(java.util.Collection)[NamedMap.getAll]) to be targetted to non-primary
partition owners (backups) to balance request load or reduce latency. If the application chooses to
target a non-primary partition owner there is an implied tolerance for stale reads. This may be
possible as the primary (or other backups) process future/in-flight changes while the targeted member
that performed the read has not.

Coherence now provides an ability for applications to choose the appropriate `read-locator` for a cache
or service via the cache configuration, as highlighted below:

[source,xml]
----
    ...
    <distributed-scheme>
      <scheme-name>example-distributed</scheme-name>
      <service-name>DistributedCache</service-name>
      <backing-map-scheme>
          <read-locator>closest</read-locator>
      </backing-map-scheme>
      <autostart>true</autostart>
    </distributed-scheme>
    ...
----

The following `read-locator` values are supported:

* `primary` - (default) target the request to the primary only.

* `closest` - find the 'closest' owner based on the machine, rack or site information for each member
in the partition's ownership chain (primary & backups).

* `random` - pick a random owner in the partition's ownership chain.

* `random-backup` - pick a random backup owner in the partition's ownership chain.

* `class-scheme` - provide your own implementation that receives the ownership chain and returns the member to target

== Scheduled Backups

As mentioned previously, Coherence provides an ability for applications to favor write throughput
over coherent backup copies (`async-backup`). This can result in acknowledged write requests being lost if
they were not successfully backed up; acknowledgement comes in the form of control being returned
when using the synchronous API against a mutating method ({javadoc-root}/com/tangosol/util/ConcurrentMap.html#put(K,V)[put] / {javadoc-root}/com/tangosol/util/InvocableMap.html#invoke(K,com.tangosol.util.InvocableMap.EntryProcessor)[invoke]), or receiving a notification
of the completion of a write request via the asynchronous API.

Internally this still results in `n` backup messages being created for `n` write requests, which
has a direct impact on write throughput. To improve write throughput Coherence now supports
"Scheduled" (or periodic) backups, thus allowing the number of backup messages to be `< n`. 

The existing `async-backup` XML element has been augmented to accept more than a simple `true|false`
value and now supports a time-based value. This allows applications to suggest a soft target of how long
they are willing to tolerate stale backups. Coherence at runtime may decide to accelerate
backup synchronicity, or increase the staleness based on primary write throughput.

[NOTE]
====
Care must be taken when choosing the backup interval, since there is a potential for losing
updates in the event of losing a primary partition owner. All the updates waiting to
be sent by that primary will not be reflected when the corresponding backup owner is restored
and becomes primary.
====

=== Example Configuration
The following distributed scheme contains an example of setting the scheduled backup interval
of ten seconds:

[source,xml]
----
    ...
    <distributed-scheme>
      <scheme-name>example-distributed</scheme-name>
      <service-name>DistributedCache</service-name>
      <autostart>true</autostart>
      <async-backup>10s</async-backup>
    </distributed-scheme>
    ...
----

A default system property can also be used, and will take effect on all distributed schemes used,
e.g.:

[source,text]
----
-Dcoherence.distributed.asyncbackup=10s
----
